<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quick Start Guide &mdash; QEfficient Documentation 1.16 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=a9e34a36"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="High level API" href="QEfficient.cloud.html" />
    <link rel="prev" title="Requirements" href="Linux_installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            QEfficient Documentation
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduciton Qualcomm Transformers Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="Validate.html">Validated Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Linux_installation.html">Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="Linux_installation.html#installation">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick start</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="#using-high-level-api">Using High Level API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#use-qefficient-cloud-infer">1. Use QEfficient.cloud.infer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#use-of-qefficient-cloud-execute">2. Use of QEfficient.cloud.execute</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#using-low-level-api">Using Low Level API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-download-and-transform">1.  Model download and transform</a></li>
<li class="toctree-l2"><a class="reference internal" href="#onnx-export-of-transformed-model">2. ONNX export of transformed model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compile-on-cloud-ai-100">3. Compile on Cloud AI 100</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run-benchmark">4. Run Benchmark</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="QEfficient.cloud.html">High level API</a></li>
<li class="toctree-l1"><a class="reference internal" href="low_level_api.html">Low level API</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_api.html">Other API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="performance.html">perfromance details</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm® Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models – Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Reference 1</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">QEfficient Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Quick Start Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/quick_start.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quick-start-guide">
<h1>Quick Start Guide<a class="headerlink" href="#quick-start-guide" title="Permalink to this heading"></a></h1>
<p>QEfficient Library was designed with one goal: <strong>to make onboarding of models inference straightforward for any Transformer architecture, while leveraging the complete power of Cloud AI platform</strong></p>
<p>To achieve this, we have 2 levels of APIs, with different levels of abstraction.</p>
<ol class="arabic simple">
<li><p>High-level APIs abstract away complex details, offering a simpler interface. They’re ideal for quick development and prototyping. If you’re new to a technology or want to minimize coding effort, high-level APIs are more user-friendly.</p></li>
<li><p>Low-level APIs offer more granular control, ideal for when customization is necessary. These are particularly useful for users who are trying their own models, not hosted on HF but are implemented based on Transformers.</p></li>
</ol>
<p>In summary:</p>
<ul class="simple">
<li><p>Choose high-level APIs for quick development, simplicity, and ease of use.</p></li>
<li><p>Opt for low-level APIs when you need fine-tuned control, optimization, or advanced customization.</p></li>
</ul>
</section>
<section id="using-high-level-api">
<h1>Using High Level API<a class="headerlink" href="#using-high-level-api" title="Permalink to this heading"></a></h1>
<section id="use-qefficient-cloud-infer">
<h2>1. Use QEfficient.cloud.infer<a class="headerlink" href="#use-qefficient-cloud-infer" title="Permalink to this heading"></a></h2>
<p>This is the single e2e python api in the library, which takes model_card name as input along with other compile args if necessary and does everything in one go.</p>
<ul class="simple">
<li><p>Torch Download → Optimize for Cloud AI 100 → Export to ONNX → Verify (CPU) → Compile on Cloud AI 100 → <a class="reference internal" href="#2-use-of-qefficientcloudexecute"><span class="xref myst">Execute</span></a></p></li>
<li><p>Its skips the ONNX export/compile stage if ONNX file or qpc found on path</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check out the options using the help menu</span>
python<span class="w"> </span>-m<span class="w"> </span>QEfficient.cloud.infer<span class="w"> </span>--help
python<span class="w"> </span>-m<span class="w"> </span>QEfficient.cloud.infer<span class="w"> </span>--model_name<span class="w"> </span>gpt2<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--prompt_len<span class="w"> </span><span class="m">32</span><span class="w"> </span>--ctx_len<span class="w"> </span><span class="m">128</span><span class="w"> </span>--mxfp6<span class="w"> </span>--num_cores<span class="w"> </span><span class="m">16</span><span class="w"> </span>--device_group<span class="w"> </span><span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w"> </span>--prompt<span class="w"> </span><span class="s2">&quot;My name is&quot;</span><span class="w"> </span>--mos<span class="w"> </span><span class="m">1</span><span class="w"> </span>--aic_enable_depth_first<span class="w">  </span>

<span class="c1"># If executing for batch size&gt;1,</span>

<span class="c1"># Either pass input prompts in single string but seperate with pipe (|) symbol&quot;. Example below</span>

python<span class="w"> </span>-m<span class="w"> </span>QEfficient.cloud.infer<span class="w"> </span>--model_name<span class="w"> </span>gpt2<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">3</span><span class="w"> </span>--prompt_len<span class="w"> </span><span class="m">32</span><span class="w"> </span>--ctx_len<span class="w"> </span><span class="m">128</span><span class="w"> </span>--num_cores<span class="w"> </span><span class="m">16</span><span class="w"> </span>--device_group<span class="w"> </span><span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w"> </span>--prompt<span class="w"> </span><span class="s2">&quot;My name is|The flat earth </span>
<span class="s2">theory is the belief that|The sun rises from&quot;</span><span class="w"> </span>--mxfp6<span class="w"> </span>--mos<span class="w"> </span><span class="m">1</span><span class="w"> </span>--aic_enable_depth_first

<span class="c1"># Or pass path of txt file with input prompts, Example below, sample txt file(prompts.txt) is present in examples folder .</span>

python<span class="w"> </span>-m<span class="w"> </span>QEfficient.cloud.infer<span class="w"> </span>--model_name<span class="w"> </span>gpt2<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">3</span><span class="w"> </span>--prompt_len<span class="w"> </span><span class="m">32</span><span class="w"> </span>--ctx_len<span class="w"> </span><span class="m">128</span><span class="w"> </span>--num_cores<span class="w"> </span><span class="m">16</span><span class="w"> </span>--device_group<span class="w"> </span><span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w"> </span>--prompts_txt_file_path<span class="w"> </span>examples/prompts.txt<span class="w"> </span>--mxfp6<span class="w"> </span>--mos<span class="w"> </span><span class="m">1</span><span class="w"> </span>--aic_enable_depth_first<span class="w">  </span>
</pre></div>
</div>
</section>
<section id="use-of-qefficient-cloud-execute">
<h2>2. Use of QEfficient.cloud.execute<a class="headerlink" href="#use-of-qefficient-cloud-execute" title="Permalink to this heading"></a></h2>
<p>Once we have compiled the QPC, we can now use the precompiled QPC in execute API to run for different prompts, like below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>QEfficient.cloud.execute<span class="w"> </span>--model_name<span class="w"> </span>gpt2<span class="w"> </span>--qpc_path<span class="w"> </span>qeff_models/gpt2/qpc_16cores_1BS_32PL_128CL_1devices_mxfp6/qpcs<span class="w"> </span>--prompt<span class="w"> </span><span class="s2">&quot;Once upon a time in&quot;</span><span class="w"> </span>--device_group<span class="w"> </span><span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w">  </span>
</pre></div>
</div>
<p>We can also enable MQ, just based on the number of devices. Based on the “–device-group” as input it will create TS config on the fly. If “–device-group [0,1]” it will create TS config for 2 devices and use it for compilation, if “–device-group 0” then TS compilation is skipped and single soc execution is enabled.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>QEfficient.cloud.infer<span class="w"> </span>--model_name<span class="w"> </span>Salesforce/codegen-2B-mono<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--prompt_len<span class="w"> </span><span class="m">32</span><span class="w"> </span>--ctx_len<span class="w"> </span><span class="m">128</span><span class="w"> </span>--mxfp6<span class="w"> </span>--num_cores<span class="w"> </span><span class="m">16</span><span class="w"> </span>--device-group<span class="w"> </span><span class="o">[</span><span class="m">0</span>,1<span class="o">]</span><span class="w"> </span>--prompt<span class="w"> </span><span class="s2">&quot;def fibonacci(n):&quot;</span><span class="w"> </span>--mos<span class="w"> </span><span class="m">2</span><span class="w"> </span>--aic_enable_depth_first<span class="w">  </span>
<span class="w"> </span>
<span class="c1"># Once qpc is saved, you can use the execute API to run for different prompts</span>
python<span class="w"> </span>-m<span class="w"> </span>QEfficient.cloud.execute<span class="w"> </span>--model_name<span class="w"> </span>Salesforce/codegen-2B-mono<span class="w"> </span>--qpc-path<span class="w"> </span>qeff_models/Salesforce/codegen-2B-mono/qpc_16cores_1BS_32PL_128CL_2devices_mxfp6/qpcs<span class="w"> </span>--prompt<span class="w"> </span><span class="s2">&quot;def binary_search(array: np.array, k: int):&quot;</span><span class="w"> </span>--device-group<span class="w"> </span><span class="o">[</span><span class="m">0</span>,1<span class="o">]</span><span class="w"> </span>
<span class="w"> </span>
<span class="c1"># To disable MQ, just pass single soc like below:</span>
python<span class="w"> </span>-m<span class="w"> </span>QEfficient.cloud.infer<span class="w"> </span>--model_name<span class="w"> </span>gpt2<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--prompt_len<span class="w"> </span><span class="m">32</span><span class="w"> </span>--ctx_len<span class="w"> </span><span class="m">128</span><span class="w"> </span>--mxfp6<span class="w"> </span>--num_cores<span class="w"> </span><span class="m">16</span><span class="w"> </span>--device-group<span class="w"> </span><span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w"> </span>--prompt<span class="w"> </span><span class="s2">&quot;My name is&quot;</span><span class="w"> </span>--mos<span class="w"> </span><span class="m">1</span><span class="w"> </span>--aic_enable_depth_first
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>High Level APIs</p></th>
<th class="head"><p>Single SoC</p></th>
<th class="head"><p>Tensor Slicing</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>QEfficient.cloud.infer</p></td>
<td><p>python -m QEfficient.cloud.infer –model_name {model_name}  –batch_size 1 –prompt_len 128 –ctx_len 1024 –num_cores 16 –device-group [0] –prompt “My name is” –mxfp6 –hf_token  {xyz}  –mos 1 –aic_enable_depth_first</p></td>
<td><p>python -m QEfficient.cloud.infer –model_name {model}  –batch_size 1 –prompt_len 128 –ctx_len 1024–num_cores 16 –device-group [0,1,2,3] –prompt “My name is” –mxfp6 –hf_token  {xyz}  –mos 4 –aic_enable_depth_first</p></td>
</tr>
<tr class="row-odd"><td><p>QEfficient.cloud.execute</p></td>
<td><p>python -m QEfficient.cloud.execute –model_name {model}  –device_group [0] –qpc_path  {path}  –prompt “My name is”  –hf_token  {xyz}</p></td>
<td><p>python -m QEfficient.cloud.execute –model_name {model}  –device_group [0,1,2,3] –qpc_path  {path}  –prompt “My name is”  –hf_token  {xyz}</p></td>
</tr>
</tbody>
</table>
<p><strong>Note: Replace {model} ,  {path}  and  {xyz}  with preffered model card name, qpc path and hf token respectively.</strong></p>
</section>
</section>
<section id="using-low-level-api">
<h1>Using Low Level API<a class="headerlink" href="#using-low-level-api" title="Permalink to this heading"></a></h1>
<section id="model-download-and-transform">
<h2>1.  Model download and transform<a class="headerlink" href="#model-download-and-transform" title="Permalink to this heading"></a></h2>
<p>Initialize QEfficient and transform the models, Check the list of supported architectures in the repo.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initiate the Orignal Transformer model</span>
import<span class="w"> </span>os
from<span class="w"> </span>transformers.models.gpt2.modeling_gpt2<span class="w"> </span>import<span class="w"> </span>GPT2LMHeadModel
import<span class="w"> </span>QEfficient
from<span class="w"> </span>transformers<span class="w"> </span>import<span class="w"> </span>AutoTokenizer
from<span class="w"> </span>QEfficient.utils<span class="w"> </span>import<span class="w"> </span>hf_download
from<span class="w"> </span>QEfficient.utils.constants<span class="w"> </span>import<span class="w"> </span>Constants
<span class="c1"># Please uncomment and use appropriate Cache Directory for transformers, in case you don&#39;t want to use default ~/.cache dir.</span>
<span class="c1"># os.environ[&quot;TRANSFORMERS_CACHE&quot;] = &quot;/local/mnt/workspace/hf_cache&quot;</span>

<span class="nv">ROOT_DIR</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>os.path.dirname<span class="o">(</span>os.path.abspath<span class="o">(</span><span class="s2">&quot;&quot;</span><span class="o">))</span>

<span class="c1"># Model-Card name to be onboarded (This is HF Model Card name) : https://huggingface.co/gpt2-xl</span>

<span class="nv">model_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;gpt2&quot;</span><span class="w"> </span>

<span class="c1"># Similar, we can change model name and generate corresponding models, if we have added the support in the lib.</span>

<span class="nv">model_hf_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>hf_download<span class="o">(</span><span class="nv">repo_id</span><span class="o">=</span>model_name,<span class="w"> </span><span class="nv">cache_dir</span><span class="o">=</span>Constants.CACHE_DIR,<span class="w"> </span><span class="nv">ignore_pattrens</span><span class="o">=[</span><span class="s2">&quot;*.txt&quot;</span>,<span class="w"> </span><span class="s2">&quot;*.onnx&quot;</span>,<span class="w"> </span><span class="s2">&quot;*.ot&quot;</span>,<span class="w"> </span><span class="s2">&quot;*.md&quot;</span>,<span class="w"> </span><span class="s2">&quot;*.tflite&quot;</span>,<span class="w"> </span><span class="s2">&quot;*.pdf&quot;</span><span class="o">])</span>
<span class="nv">model_hf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>GPT2LMHeadModel.from_pretrained<span class="o">(</span>model_hf_path,<span class="w"> </span><span class="nv">use_cache</span><span class="o">=</span>True<span class="o">)</span>
model_hf.eval<span class="o">()</span>
print<span class="o">(</span>f<span class="s2">&quot;{model_name} from hugging-face \n&quot;</span>,<span class="w"> </span>model_hf<span class="o">)</span>

<span class="c1"># Easy and minimal api to update the model</span>
<span class="nv">model_transformed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>QEfficient.transform<span class="o">(</span>model_hf,<span class="w"> </span><span class="nv">type</span><span class="o">=</span><span class="s2">&quot;Transformers&quot;</span>,<span class="w"> </span><span class="nv">form_factor</span><span class="o">=</span><span class="s2">&quot;cloud&quot;</span><span class="o">)</span>

model_transformed.eval<span class="o">()</span>
print<span class="o">(</span><span class="s2">&quot;Model after Optimized transformations \n&quot;</span>,<span class="w"> </span>model_transformed<span class="o">)</span>
</pre></div>
</div>
</section>
<section id="onnx-export-of-transformed-model">
<h2>2. ONNX export of transformed model<a class="headerlink" href="#onnx-export-of-transformed-model" title="Permalink to this heading"></a></h2>
<p>use the qualcomm_efficient_converter API to export the KV transformed Model to ONNX and Verify on Torch.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>from<span class="w"> </span>QEfficient.exporter.export_hf_to_cloud_ai_100<span class="w"> </span>import<span class="w"> </span>qualcomm_efficient_converter

<span class="c1"># We can now export the modified models to  ONNX framework</span>
<span class="c1"># This will generate single ONNX Model for both Prefill and Decode Variations which are optimized for</span>
<span class="c1"># Cloud AI 100 Platform.</span>

<span class="c1"># This will generate  ONNX model, clip the overflow constants to fp16</span>
<span class="c1"># Verify the model on  ONNXRuntime vs Pytorch</span>
<span class="c1"># Then generate inputs and custom_io.yaml file required for compilation.</span>

<span class="c1"># We can generate the KV Style models with the flag &quot;kv&quot;</span>
<span class="c1"># Bertstyle models do not have any optimization w.r.t KV cache changes and are unoptimized version.</span>
<span class="c1"># It is recommended to use kv=True for better performance.</span>

<span class="c1"># For custom models defined on the Hub in their own modeling files. We need `trust_remote_code` option</span>
<span class="c1"># Should be set to `True` in `AutoTokenizer` for repositories you trust.</span>
<span class="nv">tokenizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>AutoTokenizer.from_pretrained<span class="o">(</span>model_hf_path,<span class="w"> </span><span class="nv">use_cache</span><span class="o">=</span>True,<span class="w"> </span><span class="nv">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="o">)</span>
base_path,<span class="w"> </span><span class="nv">onnx_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>qualcomm_efficient_converter<span class="o">(</span>
<span class="w">    </span><span class="nv">model_kv</span><span class="o">=</span>model_transformed,
<span class="w">    </span><span class="nv">model_name</span><span class="o">=</span>model_name,
<span class="w">    </span><span class="nv">kv</span><span class="o">=</span>True,
<span class="w">    </span><span class="nv">form_factor</span><span class="o">=</span><span class="s2">&quot;cloud&quot;</span>,
<span class="w">    </span><span class="nv">return_path</span><span class="o">=</span>True,
<span class="w">    </span><span class="nv">tokenizer</span><span class="o">=</span>tokenizer,
<span class="o">)</span>
</pre></div>
</div>
</section>
<section id="compile-on-cloud-ai-100">
<h2>3. Compile on Cloud AI 100<a class="headerlink" href="#compile-on-cloud-ai-100" title="Permalink to this heading"></a></h2>
<p>Once, the model is exported, Compile the model on Cloud AI 100 and generate QPC.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Please use platform SDk to Check num_cores for your card.</span>
from<span class="w"> </span>QEfficient.cloud.compile<span class="w"> </span>import<span class="w"> </span>main<span class="w"> </span>as<span class="w"> </span>compile

<span class="nv">generated_qpc_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>compile<span class="o">(</span>
<span class="w">    </span><span class="nv">onnx_path</span><span class="o">=</span>onnx_path,
<span class="w">    </span><span class="nv">num_cores</span><span class="o">=</span><span class="m">14</span>,
<span class="w">    </span><span class="nv">qpc_path</span><span class="o">=</span>base_path,
<span class="w">    </span><span class="nv">device_group</span><span class="o">=[</span><span class="m">0</span><span class="o">]</span>,
<span class="w">    </span><span class="nv">mxfp6</span><span class="o">=</span>True,
<span class="o">)</span>
</pre></div>
</div>
</section>
<section id="run-benchmark">
<h2>4. Run Benchmark<a class="headerlink" href="#run-benchmark" title="Permalink to this heading"></a></h2>
<p>Benchmark the model on Cloud AI 100, run the infer API to print tokens and tok/sec</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>from<span class="w"> </span>QEfficient.generation.text_generation_inference<span class="w"> </span>import<span class="w"> </span>cloud_ai_100_exec_kv,<span class="w"> </span>get_compilation_batch_size

<span class="c1"># post compilation, we can print the latency stats for the kv models, We provide API to print token and Latency stats on AI 100</span>
<span class="c1"># We need the compiled prefill and decode qpc to compute the token generated, This is based on Greedy Sampling Approach</span>
<span class="nv">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>get_compilation_batch_size<span class="o">(</span>generated_qpc_path<span class="o">)</span>
cloud_ai_100_exec_kv<span class="o">(</span><span class="nv">batch_size</span><span class="o">=</span>batch_size,<span class="w"> </span><span class="nv">tokenizer</span><span class="o">=</span>tokenizer,<span class="w"> </span><span class="nv">qpc_path</span><span class="o">=</span>generated_qpc_path,<span class="w"> </span><span class="nv">device_id</span><span class="o">=[</span><span class="m">0</span><span class="o">]</span>,<span class="w"> </span><span class="nv">prompt</span><span class="o">=</span><span class="s2">&quot;My name is&quot;</span><span class="o">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Linux_installation.html" class="btn btn-neutral float-left" title="Requirements" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="QEfficient.cloud.html" class="btn btn-neutral float-right" title="High level API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>