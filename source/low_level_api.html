<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Low level API &mdash; Efficient-Transformers 1.16 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=a9e34a36"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Utilities" href="other_api.html" />
    <link rel="prev" title="High Level API" href="high_level_api.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Efficient-Transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduciton Qualcomm Efficient-Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="Validate.html">Validated Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Validate.html#models-coming-soon">Models Coming Soon</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Linux_installation.html">Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="Linux_installation.html#installation">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html#using-high-level-api">Using High Level API</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html#using-low-level-api">Using Low Level API</a></li>
<li class="toctree-l1"><a class="reference internal" href="kv_change.html">Details on KV Cache Optimization for Cloud AI 100</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="high_level_api.html">High Level API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Low level API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-QEfficient.src._transformers.auto">QEfficient.transform.from</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM"><code class="docutils literal notranslate"><span class="pre">QEFFAutoModelForCausalLM</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100">QEfficient.exporter.export_hf_to_cloud_ai_100 module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_bertstyle"><code class="docutils literal notranslate"><span class="pre">convert_to_cloud_bertstyle()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_kvstyle"><code class="docutils literal notranslate"><span class="pre">convert_to_cloud_kvstyle()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter"><code class="docutils literal notranslate"><span class="pre">qualcomm_efficient_converter()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-QEfficient.compile.compile_helper">QEfficient.compile</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.compile.compile_helper.compile"><code class="docutils literal notranslate"><span class="pre">compile()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-QEfficient.generation.text_generation_inference">QEfficient.generation.text_generation_inference module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv_helper"><code class="docutils literal notranslate"><span class="pre">cloud_ai_100_exec_kv_helper()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#QEfficient.generation.text_generation_inference.latency_stats_bertstyle"><code class="docutils literal notranslate"><span class="pre">latency_stats_bertstyle()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="other_api.html">Utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="blogs.html">Train anywhere, Infer on Qualcomm Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100">How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm® Cloud AI 100</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk">Power-efficient acceleration for large language models – Qualcomm Cloud AI SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats">Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html#qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities">Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Qualcomm Cloud AI home</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#qualcomm-cloud-ai-sdk-download">Qualcomm Cloud AI SDK download</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#qualcomm-cloud-ai-api-reference">Qualcomm Cloud AI API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#user-guide">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html#ocp-microscaling-formats-mx-specification">OCP Microscaling Formats (MX) Specification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Efficient-Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Low level API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/source/low_level_api.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="low-level-api">
<h1>Low level API<a class="headerlink" href="#low-level-api" title="Permalink to this heading"></a></h1>
<section id="module-QEfficient.src._transformers.auto">
<span id="qefficient-transform-from"></span><h2>QEfficient.transform.from<a class="headerlink" href="#module-QEfficient.src._transformers.auto" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">QEfficient.src._transformers.auto.</span></span><span class="sig-name descname"><span class="pre">QEFFAutoModelForCausalLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/src/_transformers/auto.html#QEFFAutoModelForCausalLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">QEFFTransformersBase</span></code></p>
<p>QEFF class which uses  QEfficient.transform to optimizing any kind of model (i.e. LLM, SD, AWQ etc.) for cloud AI 100. It replaces
the torch.nn.Module layers with passed QEffModel layers which is optimized implementation of the same.</p>
</dd></dl>

</section>
<section id="module-QEfficient.exporter.export_hf_to_cloud_ai_100">
<span id="qefficient-exporter-export-hf-to-cloud-ai-100-module"></span><h2>QEfficient.exporter.export_hf_to_cloud_ai_100 module<a class="headerlink" href="#module-QEfficient.exporter.export_hf_to_cloud_ai_100" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_bertstyle">
<span class="sig-prename descclassname"><span class="pre">QEfficient.exporter.export_hf_to_cloud_ai_100.</span></span><span class="sig-name descname"><span class="pre">convert_to_cloud_bertstyle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qeff_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM" title="QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM"><span class="pre">QEFFAutoModelForCausalLM</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_dir_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/exporter/export_hf_to_cloud_ai_100.html#convert_to_cloud_bertstyle"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_bertstyle" title="Permalink to this definition"></a></dt>
<dd><p>API to convert model to Bertstyle approach.
Bertstyle Approach:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>No Prefill/Decode separably compiled.</p></li>
<li><p>No KV retention logic.</p></li>
<li><p>KV is every time computed for all the tokens until EOS/max_length.</p></li>
</ol>
</div></blockquote>
<hr class="docutils" />
<dl class="field-list simple">
<dt class="field-odd">Model_name<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. The name of the model to be used.</p>
</dd>
<dt class="field-even">Qeff_model<span class="colon">:</span></dt>
<dd class="field-even"><p>QEFFBaseModel. Transformed KV torch model to be used</p>
</dd>
<dt class="field-odd">Tokenizer<span class="colon">:</span></dt>
<dd class="field-odd"><p>HF_AutoTokenizer. Tokenizer to prepare inputs.</p>
</dd>
<dt class="field-even">Onnx_dir_path<span class="colon">:</span></dt>
<dd class="field-even"><p>Path to save exported ONNX file.</p>
</dd>
<dt class="field-odd">Seq_len<span class="colon">:</span></dt>
<dd class="field-odd"><p>int. The length of the sequence. Default is 128.</p>
</dd>
</dl>
<dl class="simple">
<dt>Return:</dt><dd><p>Path of exported ONNX file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_kvstyle">
<span class="sig-prename descclassname"><span class="pre">QEfficient.exporter.export_hf_to_cloud_ai_100.</span></span><span class="sig-name descname"><span class="pre">convert_to_cloud_kvstyle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qeff_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM" title="QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM"><span class="pre">QEFFAutoModelForCausalLM</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_dir_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/exporter/export_hf_to_cloud_ai_100.html#convert_to_cloud_kvstyle"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_kvstyle" title="Permalink to this definition"></a></dt>
<dd><p>API change model for kv retention and export to ONNX.
KV Style Approach-</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>This architecture is particularly suitable for autoregressive tasks</p></li>
<li><p>where sequence generation involves processing one token at a time</p></li>
<li><p>And contextual information from earlier tokens is crucial for predicting the next token.</p></li>
<li><p>The inclusion of a kV cache enhances the efficiency of the decoding process, making it more computationally efficient.</p></li>
</ol>
</div></blockquote>
<hr class="docutils" />
<dl class="field-list simple">
<dt class="field-odd">Model_name<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. Hugging Face Model Card name, Example: [gpt2].</p>
</dd>
<dt class="field-even">Qeff_model<span class="colon">:</span></dt>
<dd class="field-even"><p>QEFFBaseModel. Transformed KV torch model to be used</p>
</dd>
<dt class="field-odd">Tokenizer<span class="colon">:</span></dt>
<dd class="field-odd"><p>HF_AutoTokenizer. Tokenizer to prepare inputs.</p>
</dd>
<dt class="field-even">Onnx_dir_path<span class="colon">:</span></dt>
<dd class="field-even"><p>str. The path where the model is stored. If None, the model is loaded from the default location.</p>
</dd>
<dt class="field-odd">Seq_len<span class="colon">:</span></dt>
<dd class="field-odd"><p>int. The length of the sequence. Default=128.</p>
</dd>
</dl>
<dl class="simple">
<dt>Returns:</dt><dd><p>Path of exported ONNX file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter">
<span class="sig-prename descclassname"><span class="pre">QEfficient.exporter.export_hf_to_cloud_ai_100.</span></span><span class="sig-name descname"><span class="pre">qualcomm_efficient_converter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_kv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">QEFFBaseModel</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_model_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_dir_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hf_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">form_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cloud'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/QEfficient/exporter/export_hf_to_cloud_ai_100.html#qualcomm_efficient_converter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter" title="Permalink to this definition"></a></dt>
<dd><section id="api-to-convert-torch-bert-style-and-kv-style-model-to-onnx">
<h3>API to convert torch Bert style and KV style model to ONNX.<a class="headerlink" href="#api-to-convert-torch-bert-style-and-kv-style-model-to-onnx" title="Permalink to this heading"></a></h3>
<dl class="field-list simple">
<dt class="field-odd">model_name<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. The name of the model to be used.</p>
</dd>
<dt class="field-even">model_kv<span class="colon">:</span></dt>
<dd class="field-even"><p>torch.nn.Module. Transformed KV torch model to be used.</p>
</dd>
<dt class="field-odd">local_model_dir<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. Path of local model.</p>
</dd>
<dt class="field-even">tokenizer<span class="colon">:</span></dt>
<dd class="field-even"><p>HF_AutoTokenizer. Tokenizer to prepare inputs.</p>
</dd>
<dt class="field-odd">cache_dir<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. Path of the cache directory.</p>
</dd>
<dt class="field-even">onnx_dir_path<span class="colon">:</span></dt>
<dd class="field-even"><p>str. Path to store ONNx file</p>
</dd>
<dt class="field-odd">hf_token<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. Huggingface token to access gated models. Default=None.</p>
</dd>
<dt class="field-even">seq_len<span class="colon">:</span></dt>
<dd class="field-even"><p>int. The length of the sequence. Default is 128.</p>
</dd>
<dt class="field-odd">kv<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool. If false, It will export to Bert style. Default=true.</p>
</dd>
</dl>
<dl class="simple">
<dt>Returns:   </dt><dd><p>Path of exported ONNX file.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
<section id="module-QEfficient.compile.compile_helper">
<span id="qefficient-compile"></span><h2>QEfficient.compile<a class="headerlink" href="#module-QEfficient.compile.compile_helper" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.compile.compile_helper.compile">
<span class="sig-prename descclassname"><span class="pre">QEfficient.compile.compile_helper.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qpc_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aic_enable_depth_first</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxfp6</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mxint8</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/QEfficient/compile/compile_helper.html#compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.compile.compile_helper.compile" title="Permalink to this definition"></a></dt>
<dd><section id="api-to-compile-the-onnx-model-on-cloud-ai-100-platform-with-give-config">
<h3>API to compile the Onnx Model on Cloud AI 100 Platform with give config.<a class="headerlink" href="#api-to-compile-the-onnx-model-on-cloud-ai-100-platform-with-give-config" title="Permalink to this heading"></a></h3>
<dl class="field-list simple">
<dt class="field-odd">onnx_path<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. Generated Onnx Model Path.</p>
</dd>
<dt class="field-even">qpc_path<span class="colon">:</span></dt>
<dd class="field-even"><p>str. Path for saving compiled qpc binaries.</p>
</dd>
<dt class="field-odd">num_cores<span class="colon">:</span></dt>
<dd class="field-odd"><p>int. Number of cores to compile model on.</p>
</dd>
<dt class="field-even">device_group<span class="colon">:</span></dt>
<dd class="field-even"><p>List[int]. Used for finding number of devices to compile for.</p>
</dd>
<dt class="field-odd">aic_enable_depth_first<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool. Enables DFS with default memory size, disabled by default.</p>
</dd>
<dt class="field-even">mos<span class="colon">:</span></dt>
<dd class="field-even"><p>int. Effort level to reduce the on-chip memory.</p>
</dd>
<dt class="field-odd">batch_size<span class="colon">:</span></dt>
<dd class="field-odd"><p>int. Batch size to compile the model for.</p>
</dd>
<dt class="field-even">prompt_len<span class="colon">:</span></dt>
<dd class="field-even"><p>int. prompt len for the model to compile.</p>
</dd>
<dt class="field-odd">ctx_len<span class="colon">:</span></dt>
<dd class="field-odd"><p>int. Maximum context length to compile the model.</p>
</dd>
<dt class="field-even">mxfp6<span class="colon">:</span></dt>
<dd class="field-even"><p>bool. Enable compilation for MXFP6 precision</p>
</dd>
<dt class="field-odd">mxint8<span class="colon">:</span></dt>
<dd class="field-odd"><p>Compress Present/Past KV to MXINT8 using CustomIO config, default is False.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
<section id="module-QEfficient.generation.text_generation_inference">
<span id="qefficient-generation-text-generation-inference-module"></span><h2>QEfficient.generation.text_generation_inference module<a class="headerlink" href="#module-QEfficient.generation.text_generation_inference" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv_helper">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">cloud_ai_100_exec_kv_helper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedTokenizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qpc</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctx_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_debug_logs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">write_io_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#cloud_ai_100_exec_kv_helper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv_helper" title="Permalink to this definition"></a></dt>
<dd><section id="api-to-execute-qefficient-transformed-onnx-model-on-cloud-ai-100-using-compiled-qpc-file">
<h3>API to execute QEfficient transformed ONNX model on Cloud AI 100 using compiled QPC file.<a class="headerlink" href="#api-to-execute-qefficient-transformed-onnx-model-on-cloud-ai-100-using-compiled-qpc-file" title="Permalink to this heading"></a></h3>
<dl class="field-list simple">
<dt class="field-odd">tokenizer<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">qpc<span class="colon">:</span></dt>
<dd class="field-even"><p>str.  Path to the save generated binary file after compilation.</p>
</dd>
<dt class="field-odd">prompt<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. Sample prompt for the model text generation.</p>
</dd>
<dt class="field-even">input_len<span class="colon">:</span></dt>
<dd class="field-even"><p>int. input length of prompt to get number of chunks to execute on Cloud AI 100.</p>
</dd>
<dt class="field-odd">generation_len<span class="colon">:</span></dt>
<dd class="field-odd"><p>int. Maximum context length for the model to compile.</p>
</dd>
<dt class="field-even">device_id<span class="colon">:</span></dt>
<dd class="field-even"><p>List[int]. Device Ids to be used for compilation. if len(device_id) &gt; 1, it enable multiple card setup.</p>
</dd>
<dt class="field-odd">enable_debug_logs<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool. If True, it enables debugging logs.</p>
</dd>
<dt class="field-even">stream<span class="colon">:</span></dt>
<dd class="field-even"><p>bool. If True enable streamer, which returns tokens one by one as the model generates them.</p>
</dd>
<dt class="field-odd">Write_io_dir<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">automation<span class="colon">:</span></dt>
<dd class="field-even"><p>bool. If true, it print input, output and performance stats.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="QEfficient.generation.text_generation_inference.latency_stats_bertstyle">
<span class="sig-prename descclassname"><span class="pre">QEfficient.generation.text_generation_inference.</span></span><span class="sig-name descname"><span class="pre">latency_stats_bertstyle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qpc</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[0]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/QEfficient/generation/text_generation_inference.html#latency_stats_bertstyle"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#QEfficient.generation.text_generation_inference.latency_stats_bertstyle" title="Permalink to this definition"></a></dt>
<dd><section id="api-to-execute-onnx-model-on-cpu">
<h3>API to execute ONNX model on CPU.<a class="headerlink" href="#api-to-execute-onnx-model-on-cpu" title="Permalink to this heading"></a></h3>
<dl class="field-list simple">
<dt class="field-odd">model_name<span class="colon">:</span></dt>
<dd class="field-odd"><p>str. Hugging Face Model Card name, Example: gpt2.</p>
</dd>
<dt class="field-even">qpc<span class="colon">:</span></dt>
<dd class="field-even"><p>str.  Path to the save generated binary file after compilation.</p>
</dd>
<dt class="field-odd">seq_len<span class="colon">:</span></dt>
<dd class="field-odd"><p>int. Sequence length.</p>
</dd>
<dt class="field-even">prompt<span class="colon">:</span></dt>
<dd class="field-even"><p>str. Sample prompt for the model text generation.</p>
</dd>
<dt class="field-odd">device_id<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[int]. Device Ids to be used for compilation. if devices &gt; 1, it enable multiple card setup.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="high_level_api.html" class="btn btn-neutral float-left" title="High Level API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="other_api.html" class="btn btn-neutral float-right" title="Utilities" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Qualcomm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>