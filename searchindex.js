Search.setIndex({"docnames": ["Introduction", "Linux_installation", "QEfficient", "QEfficient.cloud", "QEfficient.customop", "QEfficient.exporter", "QEfficient.generation", "QEfficient.transformers", "QEfficient.utils", "Validate", "blogs", "docs", "examples", "index", "kv_change", "low_level_api", "modules", "notebooks", "other_api", "performance", "quick_start", "reference", "scripts", "setup", "tests"], "filenames": ["Introduction.md", "Linux_installation.md", "QEfficient.md", "QEfficient.cloud.md", "QEfficient.customop.md", "QEfficient.exporter.md", "QEfficient.generation.md", "QEfficient.transformers.md", "QEfficient.utils.md", "Validate.md", "blogs.md", "docs.md", "examples.md", "index.md", "kv_change.md", "low_level_api.md", "modules.md", "notebooks.md", "other_api.md", "performance.md", "quick_start.md", "reference.md", "scripts.md", "setup.md", "tests.md"], "titles": ["Introduciton Qualcomm Efficient-Transformers", "Requirements", "QEfficient package", "High level API", "QEfficient.customop package", "QEfficient.exporter package", "QEfficient.generation package", "QEfficient.transformers package", "QEfficient.utils package", "Validated Models", "Train anywhere, Infer on Qualcomm Cloud AI 100", "docs package", "examples package", "Welcome to Efficient-Transformers Documentation!", "Details on KV Cache Optimization for Cloud AI 100", "Low level API", "&lt;no title&gt;", "notebooks package", "Other API", "perfromance details", "Quick Start Guide", "Reference 1", "scripts package", "setup module", "tests package"], "terms": {"latest": 0, "new": [0, 7, 20], "fire": [], "come": [0, 13], "soon": [0, 13], "support": [0, 1, 20], "more": [0, 4, 5, 15, 20], "popular": 0, "model": [0, 1, 7, 13, 24], "infer": [0, 2, 5, 13], "optim": [0, 13, 20], "techniqu": 0, "like": [0, 20], "continu": 0, "batch": [0, 3, 15, 20], "specul": [0, 13], "decod": [0, 5, 7, 13, 15, 20], "04": 0, "2024": 0, "initi": [0, 7, 20], "releas": 0, "effici": [1, 3, 5, 8, 15], "seamless": 0, "pre": [0, 1, 7], "train": [0, 2, 4, 13], "llm": [0, 4, 13], "anywher": [0, 13], "cloud": [0, 1, 2, 4, 7, 13, 24], "ai": [0, 1, 4, 13, 24], "develop": [0, 20], "centric": 0, "toolchain": 0, "thi": [0, 3, 4, 5, 15, 20], "provid": [0, 7, 20], "reimplement": 0, "block": [0, 7], "which": [0, 6, 15, 18, 20], "ar": [0, 4, 5, 7, 8, 20], "us": [0, 3, 4, 5, 13], "make": [0, 5, 15, 20], "function": [0, 4, 24], "highli": 0, "perform": [0, 4, 6, 15, 18, 20], "100": [0, 1, 4, 13, 24], "There": [0, 4], "sever": 0, "can": [0, 4, 7, 20], "directli": [0, 4], "from": [0, 5, 7, 15, 20, 24], "origin": 0, "form": [0, 7], "deploy": 0, "readi": 0, "For": [0, 7, 20], "other": [0, 4, 13, 20], "i": [0, 4, 8, 15, 20, 24], "comprehens": 0, "document": [0, 3, 8], "inspir": 0, "upon": [0, 20], "chang": [0, 5, 15, 20, 24], "need": [0, 4, 20], "how": [0, 13], "To": [0, 20], "": [0, 7, 8], "typic": 0, "enabl": [0, 1, 3, 5, 6, 15, 18, 20, 24], "devic": [0, 1, 3, 5, 6, 8, 15, 18, 20], "retent": [0, 5, 7, 15], "intermedi": [0, 5], "state": [0, 7], "graph": 0, "execut": [0, 2, 8, 13], "kei": [0, 7, 8], "oper": 0, "lower": 0, "precis": [0, 3, 5, 15], "replac": [0, 20], "some": 0, "mathemat": 0, "equival": [0, 4], "handl": [0, 4], "underflow": 0, "overflow": [0, 4, 20], "patcher": 0, "modul": [0, 13], "map": 0, "weight": [0, 4], "updat": [0, 20], "export": [0, 2, 13, 24], "sourc": [0, 1, 3, 4, 5, 6, 7, 8, 15, 18, 24], "onnx": [0, 4, 13, 24], "sampl": [0, 3, 6, 15, 18, 20], "exampl": [0, 3, 6, 15, 18, 20], "applic": 0, "demo": 0, "notebook": 0, "unit": 0, "test": 0, "templat": 0, "It": [0, 4, 5, 15, 20], "mandatori": 0, "each": [0, 7], "pull": 0, "request": 0, "includ": 0, "If": [0, 1, 3, 5, 6, 7, 15, 18, 20], "pr": 0, "ad": [0, 7, 20], "should": [0, 1, 4, 20], "success": 0, "post": [0, 7, 20], "part": 0, "pytorch": [0, 7, 20, 24], "onnxrt": 0, "exit": 0, "criteria": 0, "mse": 0, "between": 0, "output": [0, 4, 5, 7, 15, 18], "modifi": [0, 20], "ani": [0, 4, 20], "common": 0, "util": [0, 2, 7, 20], "all": [0, 4, 5, 15], "system": 1, "linux": 1, "o": [1, 5, 20], "ubuntu": [1, 3, 8], "rhel": 1, "aw": 1, "requisit": 1, "platform": [1, 20], "app": 1, "sdk": [13, 20], "multi": 1, "shard": 1, "bulb": [], "bash": 1, "termin": 1, "memo": [], "zsh": 1, "device_group": [1, 3, 5, 15, 20, 24], "singl": [1, 20], "quot": 1, "e": [1, 3, 15], "g": [1, 3, 4, 15], "0": [1, 3, 5, 6, 7, 15, 18, 20, 24], "creat": [1, 20, 24], "python": [1, 20], "virtual": 1, "env": 1, "activ": [1, 2, 6], "3": [1, 9, 13], "8": [1, 8], "python3": 1, "m": [1, 20], "venv": 1, "qeff_env": 1, "bin": 1, "clone": 1, "qeffici": [1, 13], "repo": [1, 20], "http": [1, 7, 20], "github": [7, 13], "com": [1, 4, 7], "quic": 1, "transform": [1, 2, 3, 5, 8, 24], "librari": [0, 1, 20], "host": [1, 20], "machin": 1, "cli": 1, "api": [1, 2, 8], "until": [1, 5, 15], "we": [1, 20], "have": [1, 7, 20], "docker": 1, "integr": 1, "pip": 1, "high": [2, 13], "main": [2, 3, 7, 15, 20], "customop": 2, "rms_norm": 2, "customrmsnorma": [2, 4], "forward": [2, 4], "customrmsnormop": [2, 4], "setup_context": [2, 4], "symbol": [2, 4, 5, 20], "export_hf_to_cloud_ai_100": [2, 13, 20], "convert_to_cloud_bertstyl": [2, 5, 15], "convert_to_cloud_kvstyl": [2, 5, 15], "convert_to_edg": [2, 5], "qualcomm_efficient_convert": [2, 5, 15, 20], "export_util": 2, "compile_kv_model_on_cloud_ai_100": [2, 5], "export_onnx": [2, 5, 24], "fix_onnx_fp16": [2, 5], "generate_input_fil": [2, 5], "remove_temp_fil": [2, 5], "run_model_on_cloud_ai_100": [2, 5], "run_model_on_ort": [2, 5], "save_onnx": [2, 5], "gener": [2, 3, 13, 20], "cloud_inf": 2, "qaicinferencesess": [2, 6], "deactiv": [2, 6], "input_nam": [2, 5, 6], "output_nam": [2, 5, 6], "run": [2, 4, 6, 13], "set_buff": [2, 6], "skip_buff": [2, 6], "text_generation_infer": [2, 13, 20], "check_batch_size_and_num_prompt": [2, 6], "cloud_ai_100_exec_kv": [2, 6, 20], "cloud_ai_100_exec_kv_help": [2, 6, 15, 18], "get_compilation_batch_s": [2, 6, 20], "latency_stats_bertstyl": [2, 6, 15, 18], "print_latency_stats_kv": [2, 6], "read_prompts_txt_fil": [2, 6], "write_io_fil": [2, 6], "modeling_attn_mask_util": 2, "qeffattentionmaskconvert": [2, 7], "cache_index": [2, 7, 8], "is_caus": [2, 7], "sliding_window": [2, 7], "to_4d": [2, 7], "modeling_output": 2, "qeffbasemodeloutputwithpast": [2, 7], "attention_mask_retainedst": [2, 7], "attent": [2, 7], "hidden_st": [2, 4, 7], "last_hidden_st": [2, 7], "past_key_valu": [2, 7, 8], "qeffbasemodeloutputwithpastandcrossattent": [2, 7], "cross_attent": [2, 7], "qeffcausallmoutputwithcrossattent": [2, 7], "logit": [2, 7], "loss": [2, 7], "qeffcausallmoutputwithpast": [2, 7], "qeffmoecausallmoutputwithpast": [2, 7], "aux_loss": [2, 7], "router_logit": [2, 7], "qeffmoemodeloutputwithpast": [2, 7], "modeling_util": 2, "modelarchitectur": [2, 7], "architectur": [2, 5, 7, 15, 20], "get_params_hash": [2, 7], "replace_module_with_qeff_lay": [2, 7], "constant": [2, 5, 15, 20], "cache_dir": [2, 3, 8, 20], "ctx_len": [2, 3, 8, 15, 20], "gb": [2, 8], "input_str": [2, 5, 8, 15], "max_qpc_limit": [2, 8], "prompt_len": [2, 3, 8, 15, 20], "seq_length": [2, 5, 8, 15], "device_util": 2, "get_available_device_id": [2, 8], "is_multi_qranium_setup_avail": [2, 8], "is_qpc_size_gt_32gb": [2, 8], "generate_input": 2, "inputhandl": [2, 8], "prepare_cloud_ai_100_input": [2, 8], "prepare_ort_input": [2, 8], "prepare_pytorch_input": [2, 8], "update_cloud_ai_100_input": [2, 8], "update_ort_input": [2, 8], "update_pytorch_input": [2, 8], "logging_util": 2, "qeffformatt": [2, 8], "format": [2, 5, 8, 13], "bold_r": [2, 8], "grei": [2, 8], "red": [2, 8], "reset": [2, 6, 8], "yellow": [2, 8], "create_logg": [2, 8], "run_util": 2, "apirunn": [2, 8], "run_hf_model_on_pytorch": [2, 8], "run_kv_model_on_cloud_ai_100": [2, 8], "run_kv_model_on_ort": [2, 8], "run_kv_model_on_pytorch": [2, 8], "run_ort_sess": [2, 8], "hf_download": [2, 8, 20], "onnx_exist": [2, 8], "type": [4, 5, 15, 20, 24], "form_factor": [5, 7, 15, 20], "param": [3, 5, 6, 7, 8, 15, 18, 24], "nn": [5, 15], "instanc": [4, 15], "str": [3, 5, 6, 7, 8, 15, 18, 24], "diffus": 15, "default": [3, 5, 15, 20], "model_nam": [3, 5, 6, 8, 15, 18, 20, 24], "qpc_path": [3, 6, 8, 15, 20], "list": [3, 5, 8, 15, 18, 20], "int": [3, 5, 6, 7, 8, 15, 18, 20], "prompt": [3, 6, 8, 15, 18, 20], "none": [3, 4, 5, 6, 7, 8, 15, 18, 24], "prompts_txt_file_path": [3, 6, 20], "home": [3, 8], "amitraj": [3, 8], "hf_token": [3, 5, 8, 15, 20], "hug": [3, 6, 15, 18, 20], "face": [3, 6, 15, 18, 20], "card": [3, 6, 8, 15, 18, 20], "name": [3, 5, 8, 15, 18, 20], "gpt2": [3, 6, 9, 15, 18, 20], "text": [3, 6, 15, 18], "path": [3, 5, 6, 8, 15, 18, 20, 24], "save": [3, 4, 6, 15, 18, 20], "binari": [3, 6, 15, 18], "file": [3, 20], "after": [3, 6, 7, 15, 18, 20, 24], "compil": 13, "id": [3, 5, 6, 8, 15, 18], "1": [0, 3, 4, 5, 6, 7, 9, 13, 15, 18], "multipl": [3, 6, 15, 18], "setup": [3, 6, 15, 18, 24], "cach": [3, 5, 7, 13, 15, 20], "dir": [3, 20], "store": [3, 4, 5, 15], "download": 13, "huggingfac": [3, 5, 7, 8, 15, 20, 24], "token": [3, 5, 6, 7, 15, 18, 20, 24], "access": [3, 5, 15], "gate": [3, 5, 15], "rm": 4, "norm": 4, "node": 4, "qti": 4, "aisw": 4, "domain": 4, "fp16": [4, 15, 20], "seen": 4, "class": [4, 5, 6, 8, 15, 24], "hidden_s": [4, 7], "ep": 4, "1e": 4, "05": [0, 4], "base": [4, 5, 6, 7, 8, 9, 15, 20], "defin": [4, 20], "comput": [4, 5, 7, 15, 20], "everi": [4, 5, 15], "call": 4, "overridden": 4, "subclass": 4, "although": 4, "recip": 4, "pass": [3, 4, 5, 7, 15, 20], "within": 4, "one": [4, 5, 6, 7, 15, 18, 20], "afterward": 4, "instead": 4, "sinc": [4, 8], "former": 4, "take": [4, 20, 24], "care": 4, "regist": 4, "hook": 4, "while": [4, 20], "latter": 4, "silent": 4, "ignor": 4, "them": [4, 6, 15, 18], "bool": [3, 4, 5, 6, 7, 8, 15, 18], "arg": [4, 7, 20], "kwarg": [4, 5], "static": 4, "tensor": [4, 5, 7, 20], "epsilon": 4, "float": [4, 5], "two": 4, "wai": 4, "usag": 4, "combin": 4, "ctx": 4, "staticmethod": 4, "def": [4, 20], "must": 4, "accept": 4, "context": [3, 4, 6, 15, 18], "first": 4, "argument": 4, "follow": 4, "number": [3, 4, 5, 6, 7, 8, 15, 18, 20], "see": [4, 7], "detail": [4, 13, 20], "2": [4, 7, 9, 13], "separ": [3, 4, 15], "input": [4, 7, 15, 18, 20], "tupl": [4, 5, 7, 8], "The": [4, 5, 7, 15, 20], "longer": 4, "you": [4, 5, 20], "also": [4, 7, 20], "overrid": [4, 8], "torch": [4, 6, 7, 20], "autograd": 4, "set": [4, 7, 8, 20, 24], "up": [4, 7, 24], "object": [4, 6, 8], "extend": 4, "arbitrari": 4, "data": 4, "retriev": 4, "dure": [3, 4, 5, 15], "backward": 4, "though": 4, "current": [4, 8], "enforc": 4, "compat": 4, "either": [4, 20], "save_for_backward": 4, "thei": [4, 20], "intend": 4, "vjp": 4, "save_for_forward": 4, "jvp": 4, "an": [4, 7], "signatur": 4, "happen": 4, "insid": 4, "oppos": 4, "graphcontext": 4, "valu": [4, 8], "model_class": [5, 15, 24], "onnx_dir_path": [5, 15], "seq_len": [5, 6, 15, 18], "32": [3, 5, 8, 15, 20], "my": [5, 8, 15, 20], "return_path": [5, 15, 20], "fals": [3, 5, 6, 15, 18], "save_fp32_onnx": [5, 15], "save_fp16_onnx": [5, 15], "true": [5, 6, 7, 8, 15, 18, 20], "bertstyl": [5, 15, 20], "approach": [5, 15, 20], "No": [5, 15], "prefil": [5, 15, 20], "logic": [5, 15], "time": [5, 15, 20], "eo": [5, 15], "max_length": [5, 15], "hf_autotoken": [5, 15], "prepar": [5, 15], "model_path": [5, 8, 15], "where": [5, 15], "load": [5, 15, 24], "locat": [5, 15, 24], "length": [3, 5, 6, 7, 15, 18], "sequenc": [5, 6, 7, 15, 18], "128": [3, 5, 15, 20], "string": [5, 7, 15, 20], "process": [5, 6, 15], "return": [5, 6, 7, 15, 18, 24], "fp32": [5, 15], "unclip": [5, 15], "version": [5, 15, 20], "delet": [5, 15], "model_kv": [5, 15, 20, 24], "particularli": [5, 15, 20], "suitabl": [5, 15], "autoregress": [5, 7, 15], "task": [5, 15], "involv": [5, 15], "And": [5, 15], "contextu": [5, 15], "inform": [5, 15], "earlier": [5, 15], "crucial": [5, 15], "predict": [5, 7, 15], "next": [5, 7, 15], "inclus": [5, 15], "enhanc": [5, 15], "computation": [5, 15], "self": [5, 7], "onnx_path": [5, 15, 20], "specializations_json": 5, "num_cor": [3, 5, 15, 20], "base_path": [5, 15, 20], "mxfp6": [3, 5, 8, 15, 20], "custom_io_path": 5, "aic_enable_depth_first": [3, 5, 15, 20], "mo": [3, 5, 15, 20], "special": 5, "json": 5, "core": [3, 5, 15], "artifact": 5, "mix": 5, "point": 5, "6": 5, "mode": 5, "custom": [5, 20], "configur": [5, 7], "effort": [3, 5, 15, 20], "level": [2, 5, 8, 13], "reduc": [3, 5, 15], "chip": [3, 5, 15], "memori": [3, 5, 15], "group": [5, 20], "successfulli": [5, 24], "pt_model": 5, "dict": [5, 6, 8], "gen_models_path": 5, "model_base_nam": 5, "ort_output": [5, 8], "ndarrai": [5, 6, 8], "pt_output": [5, 8], "np": [5, 6, 20], "onnxruntim": [5, 20], "input_files_path": 5, "input_list_fil": 5, "file_path_model": 5, "file_path_weight": 5, "file_path": 5, "onnx_symbol_def": 5, "definit": 5, "dtype": [5, 7], "consid": 5, "modelproto": 5, "union": [5, 8], "hf": [5, 8, 9, 20], "device_id": [6, 8, 15, 18, 20], "enable_debug_log": [6, 15, 18], "properti": 6, "paramet": [6, 24], "skipped_buffer_nam": 6, "batch_siz": [3, 6, 7, 15, 20], "pretrainedtoken": [6, 15, 18], "pretrainedtokenizerfast": [6, 15, 18], "input_len": [6, 15, 18], "generation_len": [6, 15, 18], "stream": [6, 15, 18], "write_io_dir": [6, 15, 18], "autom": [6, 15, 18], "get": [6, 15, 18, 24], "chunk": [6, 15, 18], "maximum": [3, 6, 15, 18], "len": [3, 6, 15, 18], "debug": [6, 15, 18], "log": [6, 8, 15, 18], "streamer": [6, 15, 18], "print": [6, 8, 15, 18, 20], "stat": [6, 15, 18, 20], "generated_text": 6, "prefill_tim": 6, "decode_perf": 6, "total_perf": 6, "total_tim": 6, "write_io_subdir": 6, "write_io_nam": 6, "include_dim": 6, "longtensor": 7, "attentionmaskconvert": 7, "A": 7, "mask": 7, "allow": 7, "causal": 7, "4d": 7, "slide": 7, "window": 7, "convert": 7, "2d": 7, "query_length": 7, "key_value_length": 7, "attention_mask_2d": 7, "expand": 7, "bsz": 7, "head_dim": 7, "shape": [7, 8], "larg": [7, 13], "neg": 7, "bia": 7, "attend": 7, "posit": 7, "attention_mask": [7, 8], "copi": 7, "blob": 7, "src": 7, "py": 7, "onli": 7, "differ": [7, 8, 20], "add": 7, "idx": 7, "kv": [7, 13, 20, 24], "floattensor": 7, "booltensor": 7, "modeloutput": 7, "mai": 7, "contain": 7, "past": [7, 8], "speed": 7, "sequenti": 7, "sequence_length": 7, "hidden": 7, "last": 7, "layer": [7, 8], "option": [3, 7, 15, 20], "when": [7, 20], "use_cach": [7, 20], "config": [7, 20], "n_layer": [7, 8], "num_head": 7, "embed_size_per_head": 7, "is_encoder_decod": 7, "addit": 7, "encoder_sequence_length": 7, "cross": 7, "output_hidden_st": 7, "embed": 7, "ha": 7, "plu": 7, "output_attent": 7, "softmax": 7, "averag": 7, "head": 7, "add_cross_attent": 7, "languag": [7, 13], "label": 7, "vocab_s": 7, "score": 7, "vocabulari": 7, "befor": [7, 24], "encod": 7, "relev": 7, "is_decod": 7, "mixtur": 7, "expert": 7, "spars": 7, "output_router_prob": 7, "add_router_prob": 7, "num_expert": 7, "raw": 7, "router": 7, "logti": 7, "moe": 7, "term": 7, "auxiliari": 7, "potenti": 7, "alia": 7, "field": 7, "factor": 7, "avail": [3, 7, 8, 15], "edg": 7, "1073741824": 8, "30": 8, "padding_shap": 8, "input_id": 8, "position_id": 8, "previou": 8, "skip": [8, 20, 24], "ort": 8, "fmt": 8, "datefmt": 8, "style": [8, 20], "valid": [8, 13, 24], "formatt": 8, "color": 8, "messag": 8, "consol": 8, "10": 8, "x1b": 8, "38": 8, "20m": 8, "levelnam": 8, "filenam": 8, "lineno": 8, "d": 8, "0m": 8, "20": 8, "33": 8, "40": 8, "31": 8, "50": 8, "1m": 8, "record": 8, "method": 8, "choos": [8, 20], "logger": 8, "model_hf": [8, 20, 24], "generated_id": 8, "capi": 8, "onnxruntime_inference_collect": 8, "inferencesess": 8, "repo_id": [8, 20], "allow_pattern": 8, "ignore_pattern": 8, "llama": [9, 24], "7b": [0, 9], "chat": 9, "13b": 9, "codellama": 9, "34b": 9, "salesforc": [9, 20], "codegen25": 9, "mono_p": 9, "xgen": 9, "8k": 9, "mpt": 9, "mistral": [0, 9], "instruct": [0, 9], "v0": [0, 9], "mixtral": [0, 9], "8x7b": [0, 9], "falcon": 9, "40b": 9, "starcoder2": 9, "15b": 9, "phi": 9, "jai": 9, "30b": 9, "gpt": 9, "j": 9, "6b": 9, "chatglm2": 9, "baichuan2": 9, "click": 10, "here": 10, "introduciton": 13, "requir": [13, 20], "guid": 13, "low": 13, "4": 13, "benchmark": 13, "content": [], "perfrom": 13, "quadrupl": 13, "spd": 13, "microsc": 13, "mx": 13, "power": [13, 20], "acceler": 13, "2x": 13, "clip": [15, 20], "mxint8": [3, 15], "16": [3, 15, 20], "comma": [3, 15], "size": [3, 15, 20], "show": 18, "inherit": 18, "wa": 20, "design": 20, "goal": 20, "onboard": 20, "straightforward": 20, "leverag": 20, "complet": 20, "achiev": 20, "abstract": 20, "awai": 20, "complex": 20, "offer": 20, "simpler": 20, "interfac": 20, "re": 20, "ideal": 20, "prototyp": 20, "technologi": 20, "want": 20, "minim": 20, "code": 20, "user": 20, "friendli": 20, "granular": 20, "control": 20, "necessari": 20, "These": 20, "who": 20, "try": 20, "own": 20, "implement": 20, "In": 20, "summari": 20, "simplic": 20, "eas": 20, "opt": 20, "fine": 20, "tune": 20, "advanc": 20, "e2": 20, "model_card": 20, "along": 20, "doe": 20, "everyth": 20, "go": 20, "verifi": 20, "cpu": 20, "Its": 20, "stage": 20, "qpc": 20, "found": 20, "check": 20, "out": 20, "help": 20, "menu": 20, "seper": 20, "pipe": 20, "below": 20, "flat": 20, "earth": 20, "theori": 20, "belief": 20, "sun": 20, "rise": 20, "Or": 20, "txt": 20, "present": 20, "folder": [20, 24], "onc": 20, "now": 20, "precompil": 20, "qeff_model": 20, "qpc_16cores_1bs_32pl_128cl_1devices_mxfp6": 20, "mq": [20, 24], "just": 20, "t": 20, "fly": 20, "soc": 20, "codegen": 20, "2b": 20, "mono": 20, "fibonacci": 20, "n": 20, "qpc_16cores_1bs_32pl_128cl_2devices_mxfp6": 20, "binary_search": 20, "arrai": 20, "k": 20, "disabl": 20, "slice": 20, "1024": 20, "xyz": 20, "note": 20, "preffer": 20, "respect": 20, "orign": 20, "import": 20, "modeling_gpt2": 20, "gpt2lmheadmodel": 20, "autotoken": [20, 24], "pleas": 20, "uncom": 20, "appropri": 20, "directori": [20, 24], "case": 20, "don": 20, "environ": [20, 24], "transformers_cach": 20, "local": 20, "mnt": 20, "workspac": [20, 24], "hf_cach": 20, "root_dir": 20, "dirnam": 20, "abspath": 20, "co": 20, "xl": 20, "similar": 20, "correspond": 20, "lib": 20, "model_hf_path": 20, "ignore_pattren": 20, "ot": 20, "md": 20, "tflite": 20, "pdf": 20, "from_pretrain": 20, "eval": 20, "f": 20, "easi": 20, "model_transform": 20, "framework": 20, "both": 20, "variat": 20, "v": 20, "Then": 20, "custom_io": 20, "yaml": 20, "flag": 20, "do": 20, "w": 20, "r": 20, "unoptim": 20, "recommend": 20, "better": 20, "hub": 20, "trust_remote_cod": 20, "repositori": [13, 20], "trust": 20, "padding_sid": 20, "left": 20, "your": 20, "generated_qpc_path": 20, "14": 20, "tok": 20, "sec": 20, "latenc": 20, "greedi": 20, "onnx_model_path": 24, "get_cloud_ai_100_token": 24, "setup_info": 24, "get_token": 24, "info": 24, "load_pytorch_model": 24, "prepare_work_dir": 24, "work_dir": 24, "work": 24, "remove_temp_dir": 24, "remov": 24, "temp": 24, "set_up": 24, "model_config": 24, "testqefficientmodel": 24, "skip_if_mq_not_en": 24, "test_method": 24, "wrapper": 24, "transform_pt_model_with_qeff": 24, "qualcomm": 13, "8b": 9, "70b": 9, "io": [], "page": [], "start": [], "donwload": 1, "sh": 1, "qeff": 1, "insa": [], "rst": [], "automodul": [], "member": []}, "objects": {"": [[15, 0, 0, "-", "QEfficient"], [11, 0, 0, "-", "docs"], [12, 0, 0, "-", "examples"], [17, 0, 0, "-", "notebooks"], [22, 0, 0, "-", "scripts"], [24, 0, 0, "-", "tests"]], "QEfficient.cloud": [[15, 0, 0, "-", "compile"], [3, 0, 0, "-", "execute"], [3, 0, 0, "-", "infer"]], "QEfficient.cloud.compile": [[15, 1, 1, "", "main"]], "QEfficient.cloud.execute": [[3, 1, 1, "", "main"]], "QEfficient.cloud.infer": [[3, 1, 1, "", "main"]], "QEfficient": [[4, 0, 0, "-", "customop"], [5, 0, 0, "-", "exporter"], [15, 1, 1, "", "transform"], [8, 0, 0, "-", "utils"], [2, 0, 0, "-", "version"]], "QEfficient.customop": [[4, 2, 1, "", "CustomRMSNormAIC"], [4, 2, 1, "", "CustomRMSNormOp"]], "QEfficient.customop.CustomRMSNormAIC": [[4, 3, 1, "", "forward"], [4, 4, 1, "", "training"]], "QEfficient.customop.CustomRMSNormOp": [[4, 3, 1, "", "forward"], [4, 3, 1, "", "setup_context"], [4, 3, 1, "", "symbolic"]], "QEfficient.exporter": [[15, 0, 0, "-", "export_hf_to_cloud_ai_100"]], "QEfficient.exporter.export_hf_to_cloud_ai_100": [[15, 1, 1, "", "convert_to_cloud_bertstyle"], [15, 1, 1, "", "convert_to_cloud_kvstyle"], [5, 1, 1, "", "convert_to_edge"], [15, 1, 1, "", "qualcomm_efficient_converter"]], "QEfficient.generation.cloud_infer": [[6, 2, 1, "", "QAICInferenceSession"]], "QEfficient.generation.cloud_infer.QAICInferenceSession": [[6, 3, 1, "", "activate"], [6, 3, 1, "", "deactivate"], [6, 5, 1, "", "input_names"], [6, 5, 1, "", "output_names"], [6, 3, 1, "", "run"], [6, 3, 1, "", "set_buffers"], [6, 3, 1, "", "skip_buffers"]], "QEfficient.generation": [[18, 0, 0, "-", "text_generation_inference"]], "QEfficient.generation.text_generation_inference": [[6, 1, 1, "", "check_batch_size_and_num_prompts"], [6, 1, 1, "", "cloud_ai_100_exec_kv"], [18, 1, 1, "", "cloud_ai_100_exec_kv_helper"], [6, 1, 1, "", "get_compilation_batch_size"], [18, 1, 1, "", "latency_stats_bertstyle"], [6, 1, 1, "", "print_latency_stats_kv"], [6, 1, 1, "", "read_prompts_txt_file"], [6, 1, 1, "", "write_io_files"]], "QEfficient.transformers": [[7, 0, 0, "-", "modeling_attn_mask_utils"], [7, 0, 0, "-", "modeling_outputs"], [7, 0, 0, "-", "modeling_utils"]], "QEfficient.transformers.modeling_attn_mask_utils": [[7, 2, 1, "", "QEffAttentionMaskConverter"]], "QEfficient.transformers.modeling_attn_mask_utils.QEffAttentionMaskConverter": [[7, 4, 1, "", "cache_index"], [7, 4, 1, "", "is_causal"], [7, 4, 1, "", "sliding_window"], [7, 3, 1, "", "to_4d"]], "QEfficient.transformers.modeling_outputs": [[7, 2, 1, "", "QEffBaseModelOutputWithPast"], [7, 2, 1, "", "QEffBaseModelOutputWithPastAndCrossAttentions"], [7, 2, 1, "", "QEffCausalLMOutputWithCrossAttentions"], [7, 2, 1, "", "QEffCausalLMOutputWithPast"], [7, 2, 1, "", "QEffMoeCausalLMOutputWithPast"], [7, 2, 1, "", "QEffMoeModelOutputWithPast"]], "QEfficient.transformers.modeling_outputs.QEffBaseModelOutputWithPast": [[7, 4, 1, "", "attention_mask_RetainedState"], [7, 4, 1, "", "attentions"], [7, 4, 1, "", "hidden_states"], [7, 4, 1, "", "last_hidden_state"], [7, 4, 1, "", "past_key_values"]], "QEfficient.transformers.modeling_outputs.QEffBaseModelOutputWithPastAndCrossAttentions": [[7, 4, 1, "", "attention_mask_RetainedState"], [7, 4, 1, "", "attentions"], [7, 4, 1, "", "cross_attentions"], [7, 4, 1, "", "hidden_states"], [7, 4, 1, "", "last_hidden_state"], [7, 4, 1, "", "past_key_values"]], "QEfficient.transformers.modeling_outputs.QEffCausalLMOutputWithCrossAttentions": [[7, 4, 1, "", "attention_mask_RetainedState"], [7, 4, 1, "", "attentions"], [7, 4, 1, "", "cross_attentions"], [7, 4, 1, "", "hidden_states"], [7, 4, 1, "", "logits"], [7, 4, 1, "", "loss"], [7, 4, 1, "", "past_key_values"]], "QEfficient.transformers.modeling_outputs.QEffCausalLMOutputWithPast": [[7, 4, 1, "", "attention_mask_RetainedState"], [7, 4, 1, "", "attentions"], [7, 4, 1, "", "hidden_states"], [7, 4, 1, "", "logits"], [7, 4, 1, "", "loss"], [7, 4, 1, "", "past_key_values"]], "QEfficient.transformers.modeling_outputs.QEffMoeCausalLMOutputWithPast": [[7, 4, 1, "", "attention_mask_RetainedState"], [7, 4, 1, "", "attentions"], [7, 4, 1, "", "aux_loss"], [7, 4, 1, "", "hidden_states"], [7, 4, 1, "", "logits"], [7, 4, 1, "", "loss"], [7, 4, 1, "", "past_key_values"], [7, 4, 1, "", "router_logits"]], "QEfficient.transformers.modeling_outputs.QEffMoeModelOutputWithPast": [[7, 4, 1, "", "attention_mask_RetainedState"], [7, 4, 1, "", "attentions"], [7, 4, 1, "", "hidden_states"], [7, 4, 1, "", "last_hidden_state"], [7, 4, 1, "", "past_key_values"], [7, 4, 1, "", "router_logits"]], "QEfficient.transformers.modeling_utils": [[7, 2, 1, "", "ModelArchitectures"], [7, 1, 1, "", "get_params_hash"], [7, 1, 1, "", "replace_module_with_qeff_layers"], [7, 1, 1, "", "transform"]], "QEfficient.transformers.modeling_utils.ModelArchitectures": [[7, 4, 1, "", "architectures"]], "QEfficient.utils": [[8, 0, 0, "-", "constants"], [8, 0, 0, "-", "device_utils"], [8, 0, 0, "-", "generate_inputs"], [8, 1, 1, "", "hf_download"], [8, 0, 0, "-", "logging_utils"], [8, 1, 1, "", "onnx_exists"], [8, 0, 0, "-", "run_utils"]], "QEfficient.utils.constants": [[8, 2, 1, "", "Constants"]], "QEfficient.utils.constants.Constants": [[8, 4, 1, "", "CACHE_DIR"], [8, 4, 1, "", "CTX_LEN"], [8, 4, 1, "", "GB"], [8, 4, 1, "", "INPUT_STRING"], [8, 4, 1, "", "MAX_QPC_LIMIT"], [8, 4, 1, "", "PROMPT_LEN"], [8, 4, 1, "", "input_str"], [8, 4, 1, "", "seq_length"]], "QEfficient.utils.device_utils": [[8, 1, 1, "", "get_available_device_id"], [8, 1, 1, "", "is_multi_qranium_setup_available"], [8, 1, 1, "", "is_qpc_size_gt_32gb"]], "QEfficient.utils.generate_inputs": [[8, 2, 1, "", "InputHandler"]], "QEfficient.utils.generate_inputs.InputHandler": [[8, 3, 1, "", "prepare_cloud_ai_100_inputs"], [8, 3, 1, "", "prepare_ort_inputs"], [8, 3, 1, "", "prepare_pytorch_inputs"], [8, 3, 1, "", "update_cloud_ai_100_inputs"], [8, 3, 1, "", "update_ort_inputs"], [8, 3, 1, "", "update_pytorch_inputs"]], "QEfficient.utils.logging_utils": [[8, 2, 1, "", "QEffFormatter"], [8, 1, 1, "", "create_logger"]], "QEfficient.utils.logging_utils.QEffFormatter": [[8, 4, 1, "", "FORMATS"], [8, 4, 1, "", "bold_red"], [8, 3, 1, "", "format"], [8, 4, 1, "", "grey"], [8, 4, 1, "", "red"], [8, 4, 1, "", "reset"], [8, 4, 1, "", "yellow"]], "QEfficient.utils.run_utils": [[8, 2, 1, "", "ApiRunner"]], "QEfficient.utils.run_utils.ApiRunner": [[8, 3, 1, "", "run_hf_model_on_pytorch"], [8, 3, 1, "", "run_kv_model_on_cloud_ai_100"], [8, 3, 1, "", "run_kv_model_on_ort"], [8, 3, 1, "", "run_kv_model_on_pytorch"], [8, 3, 1, "", "run_ort_session"]], "tests": [[24, 0, 0, "-", "utils"]], "tests.utils": [[24, 1, 1, "", "export_onnx"], [24, 1, 1, "", "get_cloud_ai_100_tokens"], [24, 1, 1, "", "get_tokenizer"], [24, 1, 1, "", "load_pytorch_model"], [24, 1, 1, "", "prepare_work_dir"], [24, 1, 1, "", "remove_temp_dir"], [24, 1, 1, "", "set_up"], [24, 1, 1, "", "skip_if_mq_not_enabled"], [24, 1, 1, "", "transform_pt_model_with_qeff"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:method", "4": "py:attribute", "5": "py:property"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"], "4": ["py", "attribute", "Python attribute"], "5": ["py", "property", "Python property"]}, "titleterms": {"introduciton": 0, "qualcomm": [0, 10], "transform": [0, 6, 7, 13, 15, 18, 20], "librari": 15, "requir": [1, 5], "instal": [1, 13], "qeffici": [2, 3, 4, 5, 6, 7, 8, 15, 18, 20], "packag": [2, 4, 5, 6, 7, 8, 11, 12, 17, 22, 24], "subpackag": 2, "submodul": [2, 4, 6, 7, 8, 24], "version": 2, "modul": [2, 4, 5, 6, 7, 8, 11, 12, 15, 17, 18, 22, 23, 24], "content": [2, 4, 5, 8, 11, 12, 17, 22, 24], "low": [15, 20], "level": [3, 15, 20], "api": [3, 5, 6, 13, 15, 18, 20], "high": [3, 20], "cloud": [3, 5, 6, 8, 10, 14, 15, 18, 20], "infer": [3, 10, 20], "execut": [3, 5, 6, 15, 18, 20], "run": [3, 5, 8, 20], "model": [3, 5, 6, 8, 9, 10, 15, 18, 20], "ai": [3, 5, 6, 7, 8, 10, 14, 15, 18, 20], "100": [3, 5, 6, 7, 8, 10, 14, 15, 18, 20], "platform": [3, 15], "customop": 4, "rms_norm": 4, "export": [5, 15, 20], "export_hf_to_cloud_ai_100": [5, 15], "convert": [5, 15], "torch": [5, 15], "bert": [5, 15], "style": [5, 15], "kv": [5, 8, 14, 15], "onnx": [5, 6, 8, 15, 18, 20], "export_util": 5, "compil": [3, 5, 6, 15, 18, 20], "pytorch": [5, 8], "clip": 5, "weight": [5, 7], "fp16": 5, "rang": 5, "save": 5, "updat": [5, 8], "gener": [5, 6, 15, 18], "input": [5, 6, 8], "file": [5, 6, 8, 15, 18], "remov": [5, 8], "temporari": 5, "runtim": 5, "": 5, "data": 5, "separ": 5, "size": 5, "i": [5, 7], "greater": 5, "than": 5, "2gb": 5, "cloud_inf": 6, "provid": 6, "buffer": 6, "map": 6, "output": [6, 8], "skip": 6, "given": [6, 8, 15], "list": 6, "name": 6, "text_generation_infer": [6, 15, 18], "us": [1, 6, 7, 8, 10, 15, 18, 20], "qpc": [6, 8, 15, 18], "cpu": [6, 15, 18], "modeling_attn_mask_util": 7, "modeling_output": 7, "modeling_util": 7, "creat": [7, 8], "hash": 7, "all": 7, "paramet": 7, "valu": 7, "e": 7, "sha256": 7, "algo": 7, "replac": 7, "nn": 7, "class": 7, "optmiz": 7, "qeff": 7, "place": 7, "some": 7, "method": 7, "equival": 7, "optim": [3, 7, 14], "util": [8, 24], "constant": 8, "device_util": 8, "generate_input": 8, "function": 8, "respons": 8, "prefil": 8, "stage": 8, "numpi": 8, "onnxrt": 8, "tensor": 8, "decod": [8, 10], "logging_util": 8, "run_util": 8, "return": 8, "token": 8, "onnxruntim": 8, "session": 8, "pass": 8, "retain": 8, "state": 8, "next": 8, "iter": 8, "check": 8, "alreadi": 8, "exist": 8, "directori": 8, "have": 8, "been": 8, "manipul": 8, "valid": 9, "train": 10, "anywher": 10, "how": 10, "quadrupl": 10, "llm": 10, "perform": [10, 13], "specul": 10, "spd": 10, "microsc": 10, "mx": 10, "format": 10, "power": 10, "effici": [0, 10, 13], "acceler": 10, "larg": 10, "languag": 10, "sdk": [1, 10], "2x": 10, "doc": 11, "exampl": 12, "welcom": 13, "document": 13, "get": 13, "start": [13, 20], "quick": [13, 20], "python": 13, "blog": 13, "refer": [13, 21], "detail": [14, 19], "cach": 14, "config": 15, "notebook": 17, "other": 18, "perfrom": 19, "guid": 20, "1": [20, 21], "2": 20, "download": [3, 20], "3": 20, "4": 20, "benchmark": 20, "script": 22, "setup": 23, "test": 24, "develop": [], "centric": [], "toolchain": [], "typic": [], "come": 9, "soon": 9, "command": 3, "from": 3, "hf": 3, "aic": 3, "github": 1, "repo": [], "repositori": 1}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 58}, "alltitles": {"Submodules": [[7, "submodules"], [8, "submodules"], [24, "submodules"], [6, "submodules"], [2, "submodules"], [4, "submodules"]], "QEfficient.transformers package": [[7, "qefficient-transformers-package"]], "QEfficient.transformers.modeling_attn_mask_utils module": [[7, "module-QEfficient.transformers.modeling_attn_mask_utils"]], "QEfficient.transformers.modeling_outputs module": [[7, "module-QEfficient.transformers.modeling_outputs"]], "QEfficient.transformers.modeling_utils module": [[7, "module-QEfficient.transformers.modeling_utils"]], "Creates a Hash of all the parameters values i.e. weights using SHA256 algo.": [[7, "creates-a-hash-of-all-the-parameters-values-i-e-weights-using-sha256-algo"]], "Replaces the transformers nn.Module classes with optmized QEff classes in place.": [[7, "replaces-the-transformers-nn-module-classes-with-optmized-qeff-classes-in-place"]], "Replaces some Transformers\u2019 methods for equivalent methods optimized for AI 100.": [[7, "replaces-some-transformers-methods-for-equivalent-methods-optimized-for-ai-100"]], "Module contents": [[8, "module-QEfficient.utils"], [11, "module-docs"], [12, "module-examples"], [17, "module-notebooks"], [22, "module-scripts"], [24, "module-tests"], [5, "module-QEfficient.exporter"], [2, "module-contents"], [4, "module-QEfficient.customop"]], "QEfficient.utils package": [[8, "qefficient-utils-package"]], "QEfficient.utils.constants module": [[8, "module-QEfficient.utils.constants"]], "QEfficient.utils.device_utils module": [[8, "module-QEfficient.utils.device_utils"]], "QEfficient.utils.generate_inputs module": [[8, "module-QEfficient.utils.generate_inputs"]], "Function responsible for creating Prefill stage numpy inputs for ONNX model to be run on Cloud AI 100.": [[8, "function-responsible-for-creating-prefill-stage-numpy-inputs-for-onnx-model-to-be-run-on-cloud-ai-100"]], "Function responsible for creating Prefill stage numpy inputs for ONNX model to be run on ONNXRT.": [[8, "function-responsible-for-creating-prefill-stage-numpy-inputs-for-onnx-model-to-be-run-on-onnxrt"]], "Function responsible for creating Prefill stage tensor inputs for PyTorch model.": [[8, "function-responsible-for-creating-prefill-stage-tensor-inputs-for-pytorch-model"]], "Function responsible for updating Prefill stage inputs to create inputs for decode stage inputs for ONNX model to be run on ONNXRT.": [[8, "function-responsible-for-updating-prefill-stage-inputs-to-create-inputs-for-decode-stage-inputs-for-onnx-model-to-be-run-on-onnxrt"]], "Function responsible for updating Prefill stage inputs to create inputs for decode stage inputs for PyTorch model.": [[8, "function-responsible-for-updating-prefill-stage-inputs-to-create-inputs-for-decode-stage-inputs-for-pytorch-model"]], "QEfficient.utils.logging_utils module": [[8, "module-QEfficient.utils.logging_utils"]], "QEfficient.utils.run_utils module": [[8, "module-QEfficient.utils.run_utils"]], "Function responsible for running ONNX model on Cloud AI 100 and return the output tokens": [[8, "function-responsible-for-running-onnx-model-on-cloud-ai-100-and-return-the-output-tokens"]], "Function responsible for running ONNX model on onnxruntime and return the output tokens": [[8, "function-responsible-for-running-onnx-model-on-onnxruntime-and-return-the-output-tokens"]], "Function responsible for running KV PyTorch model and return the output tokens": [[8, "function-responsible-for-running-kv-pytorch-model-and-return-the-output-tokens"]], "Function responsible for running onnxrt session with given inputs and passing retained state outputs to be used for next iteration inputs": [[8, "function-responsible-for-running-onnxrt-session-with-given-inputs-and-passing-retained-state-outputs-to-be-used-for-next-iteration-inputs"]], "Checks if qpc files already exists, removes the directory if files have been manipulated.": [[8, "checks-if-qpc-files-already-exists-removes-the-directory-if-files-have-been-manipulated"]], "Train anywhere, Infer on Qualcomm Cloud AI 100": [[10, "train-anywhere-infer-on-qualcomm-cloud-ai-100"]], "How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm\u00ae Cloud AI 100": [[10, "how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100"]], "Power-efficient acceleration for large language models \u2013 Qualcomm Cloud AI SDK": [[10, "power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk"]], "Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats": [[10, "qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats"]], "docs package": [[11, "docs-package"]], "examples package": [[12, "examples-package"]], "Welcome to Efficient-Transformers Documentation!": [[13, "welcome-to-efficient-transformers-documentation"]], "Getting Started": [[13, null]], "Installation": [[13, null], [1, "installation"]], "Quick start": [[13, null]], "Python API": [[13, null]], "Performance": [[13, null]], "Blogs": [[13, null]], "Reference": [[13, null]], "Details on KV Cache Optimization for Cloud AI 100": [[14, "details-on-kv-cache-optimization-for-cloud-ai-100"]], "notebooks package": [[17, "notebooks-package"]], "perfromance details": [[19, "perfromance-details"]], "Reference 1": [[21, "reference-1"]], "scripts package": [[22, "scripts-package"]], "tests package": [[24, "tests-package"]], "tests.utils module": [[24, "module-tests.utils"]], "High level API": [[3, "high-level-api"]], "QEfficient.cloud.infer": [[3, "module-QEfficient.cloud.infer"]], "Inference command, the model will be downloaded from HF, optimized, compiled, executed on AIC.": [[3, "inference-command-the-model-will-be-downloaded-from-hf-optimized-compiled-executed-on-aic"]], "QEfficient.cloud.execute": [[3, "module-QEfficient.cloud.execute"]], "API to run the model on Cloud AI 100 platform.": [[3, "api-to-run-the-model-on-cloud-ai-100-platform"]], "Validated Models": [[9, "validated-models"]], "Models Coming Soon": [[9, "models-coming-soon"]], "QEfficient.generation.text_generation_inference module": [[6, "module-QEfficient.generation.text_generation_inference"], [15, "module-QEfficient.generation.text_generation_inference"], [18, "module-QEfficient.generation.text_generation_inference"]], "API to execute QEfficient transformed ONNX model on Cloud AI 100 using compiled QPC file.": [[6, "api-to-execute-qefficient-transformed-onnx-model-on-cloud-ai-100-using-compiled-qpc-file"], [15, "api-to-execute-qefficient-transformed-onnx-model-on-cloud-ai-100-using-compiled-qpc-file"], [18, "api-to-execute-qefficient-transformed-onnx-model-on-cloud-ai-100-using-compiled-qpc-file"]], "API to execute ONNX model on CPU.": [[6, "api-to-execute-onnx-model-on-cpu"], [15, "api-to-execute-onnx-model-on-cpu"], [18, "api-to-execute-onnx-model-on-cpu"]], "QEfficient.generation package": [[6, "qefficient-generation-package"]], "QEfficient.generation.cloud_infer module": [[6, "module-QEfficient.generation.cloud_infer"]], "Execute on cloud AI 100": [[6, "execute-on-cloud-ai-100"]], "Provide buffer mapping for input and output": [[6, "provide-buffer-mapping-for-input-and-output"]], "skip buffer mapping for given list of buffer names": [[6, "skip-buffer-mapping-for-given-list-of-buffer-names"]], "QEfficient.exporter.export_hf_to_cloud_ai_100 module": [[5, "module-QEfficient.exporter.export_hf_to_cloud_ai_100"], [15, "module-QEfficient.exporter.export_hf_to_cloud_ai_100"]], "API to convert torch Bert style and KV style model to ONNX.": [[5, "api-to-convert-torch-bert-style-and-kv-style-model-to-onnx"], [15, "api-to-convert-torch-bert-style-and-kv-style-model-to-onnx"]], "QEfficient.exporter package": [[5, "qefficient-exporter-package"]], "QEfficient.exporter.export_utils module": [[5, "module-QEfficient.exporter.export_utils"]], "API to compile model Cloud AI 100.": [[5, "api-to-compile-model-cloud-ai-100"]], "API for export PyTorch model to ONNX.": [[5, "api-for-export-pytorch-model-to-onnx"]], "API to clip model weights in fp16 range and save updated clipped ONNX model.": [[5, "api-to-clip-model-weights-in-fp16-range-and-save-updated-clipped-onnx-model"]], "API to generate input files, required for Cloud AI 100 execution.": [[5, "api-to-generate-input-files-required-for-cloud-ai-100-execution"]], "API to remove a temporary file": [[5, "api-to-remove-a-temporary-file"]], "API to run model on Cloud AI 100.": [[5, "api-to-run-model-on-cloud-ai-100"]], "API to run ONNX model on ONNX runtime": [[5, "api-to-run-onnx-model-on-onnx-runtime"]], "API to save ONNX model and it\u2019s data separately if size of ONNX model is greater than 2GB.": [[5, "api-to-save-onnx-model-and-it-s-data-separately-if-size-of-onnx-model-is-greater-than-2gb"]], "QEfficient package": [[2, "qefficient-package"]], "Subpackages": [[2, "subpackages"]], "QEfficient.version module": [[2, "module-QEfficient.version"]], "Low level API": [[15, "module-QEfficient"]], "Low level apis in library": [[15, "low-level-apis-in-library"]], "QEfficient.cloud.compile": [[15, "module-QEfficient.cloud.compile"]], "API to compile the ONNX model on Cloud AI 100 platform with given config.": [[15, "api-to-compile-the-onnx-model-on-cloud-ai-100-platform-with-given-config"]], "Requirements": [[1, "requirements"]], "Using SDK": [[1, "using-sdk"]], "Using GitHub Repository": [[1, "using-github-repository"]], "Introduciton Qualcomm Efficient-Transformers": [[0, "introduciton-qualcomm-efficient-transformers"]], "Quick Start Guide": [[20, "quick-start-guide"]], "Using High Level API": [[20, "using-high-level-api"]], "1. Use QEfficient.cloud.infer": [[20, "use-qefficient-cloud-infer"]], "2. Use of QEfficient.cloud.execute": [[20, "use-of-qefficient-cloud-execute"]], "Using Low Level API": [[20, "using-low-level-api"]], "1.  Model download and transform": [[20, "model-download-and-transform"]], "2. ONNX export of transformed model": [[20, "onnx-export-of-transformed-model"]], "3. Compile on Cloud AI 100": [[20, "compile-on-cloud-ai-100"]], "4. Run Benchmark": [[20, "run-benchmark"]], "QEfficient.customop package": [[4, "qefficient-customop-package"]], "QEfficient.customop.rms_norm module": [[4, "qefficient-customop-rms-norm-module"]], "Other API": [[18, "other-api"]], "setup module": [[23, "setup-module"]]}, "indexentries": {"qefficient.version": [[2, "module-QEfficient.version"]], "module": [[2, "module-QEfficient.version"], [4, "module-QEfficient.customop"], [18, "module-QEfficient.generation.text_generation_inference"]], "customrmsnormaic (class in qefficient.customop)": [[4, "QEfficient.customop.CustomRMSNormAIC"]], "customrmsnormop (class in qefficient.customop)": [[4, "QEfficient.customop.CustomRMSNormOp"]], "qefficient.customop": [[4, "module-QEfficient.customop"]], "forward() (qefficient.customop.customrmsnormaic method)": [[4, "QEfficient.customop.CustomRMSNormAIC.forward"]], "forward() (qefficient.customop.customrmsnormop static method)": [[4, "QEfficient.customop.CustomRMSNormOp.forward"]], "setup_context() (qefficient.customop.customrmsnormop static method)": [[4, "QEfficient.customop.CustomRMSNormOp.setup_context"]], "symbolic() (qefficient.customop.customrmsnormop static method)": [[4, "QEfficient.customop.CustomRMSNormOp.symbolic"]], "training (qefficient.customop.customrmsnormaic attribute)": [[4, "QEfficient.customop.CustomRMSNormAIC.training"]], "qefficient.generation.text_generation_inference": [[18, "module-QEfficient.generation.text_generation_inference"]], "cloud_ai_100_exec_kv_helper() (in module qefficient.generation.text_generation_inference)": [[18, "QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv_helper"]], "latency_stats_bertstyle() (in module qefficient.generation.text_generation_inference)": [[18, "QEfficient.generation.text_generation_inference.latency_stats_bertstyle"]]}})