Search.setIndex({"docnames": ["README", "index", "source/Introduction", "source/Linux_installation", "source/Validate", "source/blogs", "source/high_level_api", "source/kv_change", "source/low_level_api", "source/other_api", "source/performance", "source/quick_start", "source/reference", "validation"], "filenames": ["README.md", "index.md", "source/Introduction.md", "source/Linux_installation.md", "source/Validate.md", "source/blogs.md", "source/high_level_api.md", "source/kv_change.md", "source/low_level_api.md", "source/other_api.md", "source/performance.md", "source/quick_start.md", "source/reference.md", "validation.md"], "titles": ["Docs", "Welcome to Efficient-Transformers Documentation!", "Introduciton Qualcomm Efficient-Transformers", "Requirements", "Validated Models", "Train anywhere, Infer on Qualcomm Cloud AI 100", "High Level API", "Details on KV Cache Optimization for Cloud AI 100", "Low level API", "Utilities", "Perfromance details", "Quick Start Guide", "Qualcomm Cloud AI home", "Validation of Model before and After KV Cache Optimization"], "terms": {"thi": [0, 2, 6, 8, 11], "directori": [0, 8, 11], "contain": [0, 9], "stuff": 0, "static": 0, "html": 0, "document": [0, 2], "base": [0, 4, 8, 9, 11], "sphinx": 0, "firstli": 0, "instal": 0, "packag": 0, "python3": [0, 3], "m": [0, 3, 11], "pip": [0, 3], "r": [0, 11], "requir": [0, 1, 11, 13], "txt": [0, 6, 11], "And": [0, 8], "make": [0, 2, 8, 11], "The": [0, 8, 9, 11], "basic": 0, "wai": 0, "i": [0, 2, 6, 8, 9, 11, 13], "us": [0, 1, 2, 6], "http": [0, 3, 9, 11], "server": 0, "cd": 0, "8080": 0, "you": [0, 11], "can": [0, 2, 9, 11], "visit": 0, "page": 0, "your": [0, 11], "web": 0, "browser": 0, "url": 0, "localhost": 0, "introduciton": 1, "qualcomm": 1, "valid": 1, "model": [1, 2, 3], "come": [1, 2], "soon": [1, 2], "linux": 1, "sdk": [1, 11], "github": [1, 9], "repositori": 1, "guid": 1, "high": 1, "level": [1, 13], "1": [1, 2, 4, 6, 8, 9], "qeffici": [1, 3], "cloud": [1, 2, 3, 13], "infer": [1, 2], "2": [1, 4, 9], "execut": [1, 2, 9], "low": 1, "download": 1, "onnx": [1, 2, 13], "export": [1, 2], "3": [1, 3, 4], "compil": [1, 13], "ai": [1, 2, 3, 13], "100": [1, 2, 3, 13], "4": 1, "run": 1, "benchmark": 1, "detail": [1, 11], "kv": [1, 6, 11], "cach": [1, 6, 8, 9, 11], "optim": [1, 2, 8, 11], "export_hf_to_cloud_ai_100": 1, "modul": [1, 2], "gener": [1, 6, 11, 13], "text_generation_infer": [1, 11], "util": [1, 2, 11, 13], "device_util": 1, "generate_input": 1, "run_util": 1, "modeling_attn_mask_util": 1, "modeling_output": 1, "modeling_util": 1, "train": [1, 2], "anywher": [1, 2], "how": [1, 2], "quadrupl": 1, "llm": [1, 2, 8], "decod": [1, 2, 8, 11, 13], "perform": [1, 2, 8, 11], "specul": [1, 2], "spd": 1, "microsc": 1, "mx": 1, "format": 1, "power": [1, 11], "acceler": 1, "larg": [1, 9], "languag": [1, 9], "2x": 1, "introduc": 1, "One": 1, "infinit": 1, "possibl": 1, "home": [1, 6], "user": [1, 11], "ocp": 1, "specif": 1, "latest": 2, "new": [2, 9, 11], "support": [2, 3, 11], "more": [2, 8, 11], "popular": 2, "techniqu": 2, "like": [2, 11], "continu": 2, "batch": [2, 6, 8, 11], "06": 2, "2024": 2, "ad": [2, 9, 11], "gpt": [2, 4], "j": [2, 4], "6b": [2, 4], "qwen2": [2, 4], "5b": [2, 4], "instruct": [2, 4], "starcoder2": [2, 4], "15b": [2, 4], "phi3": [2, 4], "mini": [2, 4], "4k": [2, 4], "codestr": [2, 4], "22b": [2, 4], "v0": [2, 4], "vicuna": [2, 4], "v1": [2, 4], "5": [2, 4], "05": 2, "mixtral": [2, 4], "8x7b": [2, 4], "mistral": [2, 4], "7b": [2, 4], "04": 2, "initi": [2, 9, 11, 13], "releas": 2, "seamless": 2, "pre": [2, 3, 9], "develop": [2, 11], "centric": 2, "toolchain": 2, "librari": [2, 3, 11], "provid": [2, 9, 11], "reimplement": 2, "block": [2, 9], "which": [2, 8, 11, 13], "ar": [2, 9, 11, 13], "function": 2, "highli": 2, "There": 2, "sever": 2, "directli": 2, "from": [2, 8, 9, 11], "origin": 2, "form": 2, "deploy": 2, "readi": 2, "For": [2, 9], "other": [2, 11], "comprehens": 2, "inspir": 2, "upon": [2, 11], "chang": [2, 11], "need": [2, 11], "To": [2, 11, 13], "": [2, 9], "typic": 2, "enabl": [2, 3, 6, 8, 11], "devic": [2, 3, 6, 8, 9, 11, 13], "retent": [2, 8, 9], "intermedi": 2, "state": 2, "graph": 2, "kei": [2, 9, 13], "oper": 2, "lower": 2, "precis": [2, 6, 8], "replac": [2, 8], "some": 2, "mathemat": 2, "equival": 2, "handl": 2, "underflow": 2, "overflow": [2, 11], "patcher": 2, "map": 2, "weight": [2, 6, 9], "updat": 2, "sourc": [2, 3, 6, 8, 9], "sampl": [2, 6, 8, 11], "exampl": [2, 6, 8, 11], "applic": 2, "demo": 2, "notebook": 2, "unit": 2, "test": [2, 13], "templat": 2, "It": [2, 8, 11], "mandatori": 2, "each": [2, 9], "pull": 2, "request": 2, "includ": 2, "If": [2, 3, 6, 8, 9, 11], "pr": 2, "should": [2, 3], "success": 2, "post": [2, 9, 11], "part": [2, 13], "pytorch": [2, 11, 13], "onnxrt": 2, "exit": 2, "criteria": 2, "mse": 2, "between": 2, "output": [2, 8, 13], "modifi": [2, 11], "ani": [2, 8, 11], "common": 2, "all": [2, 8], "system": 3, "o": [3, 11], "ubuntu": [3, 6], "rhel": 3, "aw": 3, "requisit": 3, "platform": [3, 11], "app": [3, 11], "multi": 3, "shard": 3, "bash": 3, "termin": 3, "zsh": 3, "device_group": [3, 6, 8, 11], "singl": [3, 6, 11], "quot": 3, "e": [3, 6, 8], "g": [3, 6], "0": [3, 6, 8, 9, 11], "donwload": 3, "sh": 3, "qeff": [3, 8], "creat": [3, 11], "python": [3, 11], "virtual": 3, "env": 3, "activ": 3, "8": 3, "venv": 3, "qeff_env": 3, "bin": 3, "clone": 3, "repo": [3, 11], "git": 3, "com": [3, 9], "quic": 3, "effici": [3, 6, 8], "transform": [3, 6], "host": [3, 11], "machin": 3, "cli": 3, "api": [3, 9], "until": [3, 8], "we": [3, 11], "have": [3, 9, 11], "docker": 3, "integr": 3, "gpt2": [4, 6, 8, 11], "llama": 4, "8b": 4, "70b": 4, "chat": 4, "hf": [4, 11], "13b": 4, "codellama": 4, "34b": 4, "salesforc": [4, 11], "codegen25": 4, "mono_p": 4, "xgen": 4, "8k": 4, "mpt": 4, "falcon": 4, "40b": 4, "jai": 4, "30b": 4, "chatglm2": 4, "baichuan2": 4, "click": 5, "here": 5, "main": [6, 9], "model_nam": [6, 8, 11], "str": [6, 8, 9], "num_cor": [6, 8, 11], "int": [6, 8, 9, 11], "prompt": [6, 8, 9, 11], "none": [6, 8, 9], "local_model_dir": [6, 8], "prompts_txt_file_path": [6, 11], "aic_enable_depth_first": [6, 8, 11], "bool": [6, 8, 9], "fals": [6, 8, 11], "mo": [6, 8, 11], "cache_dir": [6, 8, 11], "hf_token": [6, 8], "batch_siz": [6, 8, 9, 11], "prompt_len": [6, 8, 9, 11, 13], "32": [6, 8, 11], "ctx_len": [6, 8, 9, 11, 13], "128": [6, 8, 11], "generation_len": [6, 8], "mxfp6": [6, 8, 11], "mxint8": [6, 8], "list": [6, 8, 9, 11], "hug": [6, 8], "face": [6, 8], "card": [6, 8, 11], "name": [6, 8, 11], "number": [6, 8, 9, 11, 13], "core": [6, 8], "text": [6, 8], "path": [6, 8, 11], "custom": [6, 11], "config": [6, 9, 11], "file": [6, 11], "take": [6, 11], "input": [6, 8, 11, 13], "pass": [6, 8, 11], "option": [6, 9, 11], "dure": 6, "default": [6, 8, 11], "effort": [6, 8, 11], "reduc": [6, 8], "chip": [6, 8], "memori": [6, 8], "dir": [6, 11], "store": [6, 8], "huggingfac": [6, 8, 9, 11, 13], "token": [6, 8, 11, 13], "access": [6, 8], "gate": [6, 8], "size": [6, 8, 11], "len": [6, 8], "maximum": [6, 8], "context": [6, 8], "length": [6, 8, 9], "compress": [6, 8], "present": [6, 8, 11, 13], "past": [6, 8, 9, 13], "customio": [6, 8, 11], "id": [6, 8, 9, 13], "comma": 6, "separ": [6, 8], "multipl": [6, 8], "setup": [6, 8], "qpc_path": [6, 8, 11, 13], "qeff_doc": 6, "documents_main": 6, "save": [6, 8, 11], "binari": [6, 8], "after": [6, 8, 9], "class": [8, 9, 13], "src": [1, 9], "_transform": 1, "auto": 1, "qeffautomodelforcausallm": [1, 11], "true": [8, 9, 11], "qefftransformersbas": 8, "kind": 8, "sd": 8, "awq": 8, "etc": 8, "nn": 8, "layer": [8, 9, 13], "qeffmodel": 8, "implement": [8, 11], "same": 8, "convert_to_cloud_bertstyl": 8, "qeff_model": [8, 11], "pretrainedtoken": 8, "pretrainedtokenizerfast": 8, "onnx_dir_path": 8, "seq_len": 8, "approach": [8, 11], "No": 8, "prefil": [8, 11, 13], "logic": 8, "everi": 8, "time": [8, 11], "comput": [8, 9, 11], "eo": 8, "max_length": 8, "union": 8, "sequenc": [8, 9], "return": [8, 13], "convert_to_cloud_kvstyl": 8, "architectur": [8, 9, 11], "particularli": [8, 11], "suitabl": 8, "autoregress": [8, 9], "task": 8, "where": 8, "involv": 8, "process": 8, "one": [8, 9, 11], "contextu": 8, "inform": 8, "earlier": 8, "crucial": 8, "predict": [8, 9], "next": 8, "inclus": 8, "enhanc": 8, "computation": 8, "qualcomm_efficient_convert": [8, 11], "model_kv": [8, 11, 13], "qeffbasemodel": 8, "seq_length": 8, "form_factor": [8, 11], "tupl": [8, 9], "local": [8, 11], "hardwar": 8, "current": [8, 9], "onli": [8, 9, 13], "accept": 8, "compile_help": 8, "onnx_path": [8, 11], "kwarg": 8, "find": 8, "df": 8, "disabl": [8, 11], "cloud_ai_100_exec_kv": [8, 11], "device_id": [8, 11, 13], "enable_debug_log": 8, "stream": 8, "write_io_dir": 8, "autom": 8, "get": 8, "chunk": 8, "debug": 8, "log": 8, "streamer": 8, "them": 8, "write": 8, "print": [8, 11], "stat": [8, 11], "latency_stats_bertstyl": 8, "get_available_device_id": 9, "check": [9, 11, 13], "avail": [9, 13], "inputhandl": [9, 13], "input_str": [9, 13], "object": 9, "prepare_cloud_ai_100_input": 9, "n_layer": [9, 13], "padding_shap": [9, 13], "shape": [9, 13], "valu": [9, 13], "dict": 9, "input_id": 9, "position_id": 9, "past_key_valu": 9, "prepare_ort_input": 9, "prepare_pytorch_input": 9, "update_cloud_ai_100_input": 9, "previou": 9, "cache_index": 9, "sinc": 9, "attention_mask": 9, "skip": [9, 11], "update_ort_input": 9, "ort_output": 9, "ort": 9, "update_pytorch_input": 9, "pt_output": 9, "apirunn": [9, 13], "run_hf_model_on_pytorch": [9, 13], "model_hf": [9, 13], "param": [9, 11, 13], "generated_id": 9, "ndarrai": 9, "run_kv_model_on_cloud_ai_100": [9, 13], "qaicinferencesess": 9, "run_kv_model_on_ort": [9, 13], "model_path": [9, 13], "run_kv_model_on_pytorch": [9, 13], "run_ort_sess": 9, "capi": 9, "onnxruntime_inference_collect": 9, "inferencesess": 9, "qeffattentionmaskconvert": 9, "is_caus": 9, "sliding_window": 9, "longtensor": 9, "attentionmaskconvert": 9, "A": 9, "attent": 9, "mask": 9, "allow": 9, "causal": 9, "4d": 9, "slide": 9, "window": 9, "convert": 9, "2d": 9, "query_length": 9, "key_value_length": 9, "to_4d": 9, "attention_mask_2d": 9, "dtype": 9, "expand": 9, "bsz": 9, "head_dim": 9, "neg": 9, "bia": 9, "attend": 9, "posit": 9, "also": [9, 11], "copi": 9, "blob": 9, "py": 9, "differ": [9, 11], "add": 9, "arg": [9, 11], "idx": 9, "qeffbasemodeloutputwithpast": 9, "last_hidden_st": 9, "floattensor": 9, "hidden_st": 9, "attention_mask_retainedst": 9, "booltensor": 9, "modeloutput": 9, "mai": 9, "speed": 9, "up": 9, "sequenti": 9, "torch": [9, 11], "sequence_length": 9, "hidden_s": 9, "hidden": 9, "last": 9, "when": [9, 11, 13], "use_cach": [9, 11], "num_head": 9, "embed_size_per_head": 9, "is_encoder_decod": 9, "addit": 9, "encoder_sequence_length": 9, "self": 9, "cross": 9, "see": 9, "output_hidden_st": 9, "embed": 9, "ha": 9, "an": 9, "plu": 9, "output_attent": 9, "softmax": 9, "averag": 9, "head": 9, "qeffbasemodeloutputwithpastandcrossattent": 9, "cross_attent": 9, "add_cross_attent": 9, "qeffcausallmoutputwithcrossattent": 9, "loss": 9, "logit": 9, "label": 9, "vocab_s": 9, "score": 9, "vocabulari": 9, "befor": 9, "encod": 9, "set": 9, "relev": 9, "is_decod": 9, "qeffcausallmoutputwithpast": 9, "qeffmoecausallmoutputwithpast": 9, "aux_loss": 9, "router_logit": 9, "mixtur": 9, "expert": 9, "spars": 9, "output_router_prob": 9, "add_router_prob": 9, "num_expert": 9, "raw": 9, "router": 9, "logti": 9, "moe": 9, "term": 9, "auxiliari": 9, "qeffmoemodeloutputwithpast": 9, "potenti": 9, "modelarchitectur": 9, "alia": 9, "field": 9, "wa": 11, "design": 11, "goal": 11, "onboard": 11, "straightforward": 11, "while": 11, "leverag": 11, "complet": 11, "achiev": 11, "abstract": 11, "awai": 11, "complex": 11, "offer": 11, "simpler": 11, "interfac": 11, "thei": 11, "re": 11, "ideal": 11, "prototyp": 11, "technologi": 11, "want": 11, "minim": 11, "code": 11, "friendli": 11, "granular": 11, "control": 11, "necessari": 11, "These": [11, 13], "who": 11, "try": 11, "own": 11, "In": 11, "summari": 11, "choos": 11, "simplic": 11, "eas": 11, "opt": 11, "fine": 11, "tune": 11, "advanc": 11, "e2": 11, "model_card": 11, "along": 11, "doe": 11, "everyth": 11, "go": 11, "verifi": 11, "cpu": 11, "stage": [11, 13], "qpc": [11, 13], "found": 11, "given": 11, "out": 11, "help": 11, "menu": 11, "16": 11, "my": 11, "either": 11, "string": 11, "seper": 11, "pipe": 11, "symbol": 11, "below": 11, "flat": 11, "earth": 11, "theori": 11, "belief": 11, "sun": 11, "rise": 11, "Or": 11, "folder": [11, 13], "onc": 11, "now": 11, "precompil": 11, "qpc_16cores_1bs_32pl_128cl_1devices_mxfp6": 11, "mq": 11, "just": 11, "t": 11, "fly": 11, "soc": 11, "codegen": 11, "2b": 11, "mono": 11, "group": 11, "def": 11, "fibonacci": 11, "n": 11, "qpc_16cores_1bs_32pl_128cl_2devices_mxfp6": 11, "binary_search": 11, "arrai": 11, "np": 11, "k": 11, "orign": 11, "import": 11, "automodelforcausallm": 11, "pleas": 11, "uncom": 11, "appropri": 11, "case": 11, "don": 11, "environ": 11, "transformers_cach": 11, "mnt": 11, "workspac": 11, "hf_cach": 11, "root_dir": 11, "dirnam": 11, "abspath": 11, "join": 11, "tmp": 11, "locat": 11, "co": 11, "xl": 11, "similar": 11, "correspond": 11, "lib": 11, "from_pretrain": 11, "f": 11, "optmiz": 11, "load_hf_token": 11, "framework": [11, 13], "both": 11, "variat": 11, "clip": 11, "constant": [11, 13], "fp16": 11, "onnxruntim": [11, 13], "v": 11, "Then": 11, "yaml": 11, "style": 11, "flag": 11, "bertstyl": 11, "do": 11, "w": 11, "unoptim": 11, "version": 11, "recommend": 11, "better": 11, "base_path": 11, "generated_qpc_path": 11, "14": 11, "qti": 11, "aic": 11, "tool": 11, "qaic": 11, "grep": 11, "nsp": 11, "total": 11, "tok": 11, "sec": 11, "get_compilation_dim": 11, "latenc": 11, "greedi": 11, "call": 13, "insid": 13, "input_gener": 13, "script": 13, "first": 13, "iter": 13, "zero": 13, "li": 13, "run_api": 13, "pytorch_hf_token": 13, "pytorch_kv_token": 13, "ort_token": 13, "onnx_model_path": 13, "cloud_ai_100_token": 13, "session": 13, "compar": 13, "cloud_ai_100": 13}, "objects": {"QEfficient.cloud": [[6, 0, 0, "-", "execute"], [6, 0, 0, "-", "infer"]], "QEfficient.cloud.execute": [[6, 1, 1, "", "main"]], "QEfficient.cloud.infer": [[6, 1, 1, "", "main"]], "QEfficient.compile": [[8, 0, 0, "-", "compile_helper"]], "QEfficient.compile.compile_helper": [[8, 1, 1, "", "compile"]], "QEfficient.exporter": [[8, 0, 0, "-", "export_hf_to_cloud_ai_100"]], "QEfficient.exporter.export_hf_to_cloud_ai_100": [[8, 1, 1, "", "convert_to_cloud_bertstyle"], [8, 1, 1, "", "convert_to_cloud_kvstyle"], [8, 1, 1, "", "qualcomm_efficient_converter"]], "QEfficient.generation": [[8, 0, 0, "-", "text_generation_inference"]], "QEfficient.generation.text_generation_inference": [[8, 1, 1, "", "cloud_ai_100_exec_kv"], [8, 1, 1, "", "latency_stats_bertstyle"]], "QEfficient.src._transformers": [[8, 0, 0, "-", "auto"]], "QEfficient.src._transformers.auto": [[8, 2, 1, "", "QEFFAutoModelForCausalLM"]], "QEfficient.transformers": [[9, 0, 0, "-", "modeling_attn_mask_utils"], [9, 0, 0, "-", "modeling_outputs"], [9, 0, 0, "-", "modeling_utils"]], "QEfficient.transformers.modeling_attn_mask_utils": [[9, 2, 1, "", "QEffAttentionMaskConverter"]], "QEfficient.transformers.modeling_attn_mask_utils.QEffAttentionMaskConverter": [[9, 3, 1, "", "cache_index"], [9, 3, 1, "", "is_causal"], [9, 3, 1, "", "sliding_window"], [9, 4, 1, "", "to_4d"]], "QEfficient.transformers.modeling_outputs": [[9, 2, 1, "", "QEffBaseModelOutputWithPast"], [9, 2, 1, "", "QEffBaseModelOutputWithPastAndCrossAttentions"], [9, 2, 1, "", "QEffCausalLMOutputWithCrossAttentions"], [9, 2, 1, "", "QEffCausalLMOutputWithPast"], [9, 2, 1, "", "QEffMoeCausalLMOutputWithPast"], [9, 2, 1, "", "QEffMoeModelOutputWithPast"]], "QEfficient.transformers.modeling_utils": [[9, 2, 1, "", "ModelArchitectures"]], "QEfficient.transformers.modeling_utils.ModelArchitectures": [[9, 3, 1, "", "architectures"]], "QEfficient.utils": [[9, 0, 0, "-", "device_utils"], [9, 0, 0, "-", "generate_inputs"], [9, 0, 0, "-", "run_utils"]], "QEfficient.utils.device_utils": [[9, 1, 1, "", "get_available_device_id"]], "QEfficient.utils.generate_inputs": [[9, 2, 1, "", "InputHandler"]], "QEfficient.utils.generate_inputs.InputHandler": [[9, 4, 1, "", "prepare_cloud_ai_100_inputs"], [9, 4, 1, "", "prepare_ort_inputs"], [9, 4, 1, "", "prepare_pytorch_inputs"], [9, 4, 1, "", "update_cloud_ai_100_inputs"], [9, 4, 1, "", "update_ort_inputs"], [9, 4, 1, "", "update_pytorch_inputs"]], "QEfficient.utils.run_utils": [[9, 2, 1, "", "ApiRunner"]], "QEfficient.utils.run_utils.ApiRunner": [[9, 4, 1, "", "run_hf_model_on_pytorch"], [9, 4, 1, "", "run_kv_model_on_cloud_ai_100"], [9, 4, 1, "", "run_kv_model_on_ort"], [9, 4, 1, "", "run_kv_model_on_pytorch"], [9, 4, 1, "", "run_ort_session"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:attribute", "4": "py:method"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "method", "Python method"]}, "titleterms": {"doc": 0, "build": 0, "preview": 0, "local": 0, "welcom": 1, "effici": [1, 2, 5], "transform": [1, 2, 5, 8, 9, 11], "document": 1, "get": 1, "start": [1, 11], "instal": [1, 3], "quick": [1, 11], "python": 1, "api": [1, 5, 6, 8, 11, 12, 13], "blog": 1, "refer": [1, 12], "introduciton": 2, "qualcomm": [2, 5, 12], "requir": 3, "linux": 3, "us": [3, 5, 8, 9, 11], "sdk": [3, 5, 12], "github": 3, "repositori": 3, "valid": [4, 13], "model": [4, 5, 6, 8, 9, 11, 13], "come": 4, "soon": 4, "train": 5, "anywher": 5, "infer": [5, 6, 11], "cloud": [5, 6, 7, 8, 9, 11, 12], "ai": [5, 6, 7, 8, 9, 11, 12], "100": [5, 6, 7, 8, 9, 11], "how": 5, "quadrupl": 5, "llm": 5, "decod": [5, 9], "perform": 5, "specul": 5, "spd": 5, "microsc": [5, 12], "mx": [5, 12], "format": [5, 12], "power": 5, "acceler": 5, "larg": 5, "languag": 5, "2x": 5, "introduc": 5, "One": 5, "infinit": 5, "possibl": 5, "high": [6, 11], "level": [6, 8, 11], "qeffici": [6, 8, 9, 11], "command": 6, "download": [6, 11, 12], "from": 6, "hf": 6, "optim": [6, 7, 13], "compil": [6, 8, 11], "execut": [6, 8, 11], "aic": 6, "run": [6, 9, 11, 13], "platform": [6, 8], "detail": [7, 10], "kv": [7, 8, 9, 13], "cach": [7, 13], "low": [8, 11], "export": [8, 11], "export_hf_to_cloud_ai_100": 8, "modul": [8, 9], "convert": 8, "torch": 8, "bert": 8, "style": 8, "onnx": [8, 9, 11], "given": [8, 9], "config": 8, "gener": 8, "text_generation_infer": 8, "qpc": 8, "file": 8, "bertstyl": 8, "util": 9, "device_util": 9, "generate_input": 9, "function": 9, "respons": 9, "creat": 9, "prefil": 9, "stage": 9, "numpi": 9, "input": 9, "onnxrt": 9, "tensor": 9, "pytorch": 9, "updat": 9, "run_util": 9, "return": 9, "output": 9, "token": 9, "onnxruntim": 9, "session": 9, "pass": 9, "retain": 9, "state": 9, "next": 9, "iter": 9, "modeling_attn_mask_util": 9, "modeling_output": 9, "modeling_util": 9, "perfrom": 10, "guid": [11, 12], "1": 11, "2": 11, "3": 11, "4": 11, "benchmark": 11, "home": 12, "user": 12, "ocp": 12, "specif": 12, "befor": 13, "after": 13, "sampl": 13, "usag": 13, "src": 8, "_transform": 8, "auto": 8, "qeffautomodelforcausallm": 8}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 58}, "alltitles": {"Docs": [[0, "docs"]], "Build the docs": [[0, "build-the-docs"]], "Preview the docs locally": [[0, "preview-the-docs-locally"]], "Welcome to Efficient-Transformers Documentation!": [[1, "welcome-to-efficient-transformers-documentation"]], "Getting Started": [[1, null]], "Installation": [[1, null]], "Quick start": [[1, null]], "Python API": [[1, null]], "Blogs": [[1, null]], "Reference": [[1, null]], "Introduciton Qualcomm Efficient-Transformers": [[2, "introduciton-qualcomm-efficient-transformers"]], "Requirements": [[3, "requirements"]], "Linux Installation": [[3, "linux-installation"]], "Using SDK": [[3, "using-sdk"]], "Using GitHub Repository": [[3, "using-github-repository"]], "Validated Models": [[4, "validated-models"]], "Models Coming Soon": [[4, "models-coming-soon"]], "Train anywhere, Infer on Qualcomm Cloud AI 100": [[5, "train-anywhere-infer-on-qualcomm-cloud-ai-100"]], "How to Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats on Qualcomm\u00ae Cloud AI 100": [[5, "how-to-quadruple-llm-decoding-performance-with-speculative-decoding-spd-and-microscaling-mx-formats-on-qualcomm-cloud-ai-100"]], "Power-efficient acceleration for large language models \u2013 Qualcomm Cloud AI SDK": [[5, "power-efficient-acceleration-for-large-language-models-qualcomm-cloud-ai-sdk"]], "Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats": [[5, "qualcomm-cloud-ai-100-accelerates-large-language-model-inference-by-2x-using-microscaling-mx-formats"]], "Qualcomm Cloud AI Introduces Efficient Transformers: One API, Infinite Possibilities": [[5, "qualcomm-cloud-ai-introduces-efficient-transformers-one-api-infinite-possibilities"]], "High Level API": [[6, "high-level-api"]], "QEfficient.cloud.infer": [[6, "module-QEfficient.cloud.infer"]], "Inference command, the model will be downloaded from HF, optimized, compiled, executed on AIC.": [[6, "inference-command-the-model-will-be-downloaded-from-hf-optimized-compiled-executed-on-aic"]], "QEfficient.cloud.execute": [[6, "module-QEfficient.cloud.execute"]], "API to run the model on Cloud AI 100 platform.": [[6, "api-to-run-the-model-on-cloud-ai-100-platform"]], "Details on KV Cache Optimization for Cloud AI 100": [[7, "details-on-kv-cache-optimization-for-cloud-ai-100"]], "Utilities": [[9, "utilities"]], "QEfficient.utils.device_utils module": [[9, "module-QEfficient.utils.device_utils"]], "QEfficient.utils.generate_inputs module": [[9, "module-QEfficient.utils.generate_inputs"]], "Function responsible for creating Prefill stage numpy inputs for ONNX model to be run on Cloud AI 100.": [[9, "function-responsible-for-creating-prefill-stage-numpy-inputs-for-onnx-model-to-be-run-on-cloud-ai-100"]], "Function responsible for creating Prefill stage numpy inputs for ONNX model to be run on ONNXRT.": [[9, "function-responsible-for-creating-prefill-stage-numpy-inputs-for-onnx-model-to-be-run-on-onnxrt"]], "Function responsible for creating Prefill stage tensor inputs for PyTorch model.": [[9, "function-responsible-for-creating-prefill-stage-tensor-inputs-for-pytorch-model"]], "Function responsible for updating Prefill stage inputs to create inputs for decode stage inputs for ONNX model to be run on ONNXRT.": [[9, "function-responsible-for-updating-prefill-stage-inputs-to-create-inputs-for-decode-stage-inputs-for-onnx-model-to-be-run-on-onnxrt"], [9, "id1"]], "Function responsible for updating Prefill stage inputs to create inputs for decode stage inputs for PyTorch model.": [[9, "function-responsible-for-updating-prefill-stage-inputs-to-create-inputs-for-decode-stage-inputs-for-pytorch-model"]], "QEfficient.utils.run_utils module": [[9, "module-QEfficient.utils.run_utils"]], "Function responsible for running ONNX model on Cloud AI 100 and return the output tokens": [[9, "function-responsible-for-running-onnx-model-on-cloud-ai-100-and-return-the-output-tokens"]], "Function responsible for running ONNX model on onnxruntime and return the output tokens": [[9, "function-responsible-for-running-onnx-model-on-onnxruntime-and-return-the-output-tokens"]], "Function responsible for running KV PyTorch model and return the output tokens": [[9, "function-responsible-for-running-kv-pytorch-model-and-return-the-output-tokens"]], "Function responsible for running onnxrt session with given inputs and passing retained state outputs to be used for next iteration inputs": [[9, "function-responsible-for-running-onnxrt-session-with-given-inputs-and-passing-retained-state-outputs-to-be-used-for-next-iteration-inputs"]], "QEfficient.transformers.modeling_attn_mask_utils module": [[9, "module-QEfficient.transformers.modeling_attn_mask_utils"]], "QEfficient.transformers.modeling_outputs module": [[9, "module-QEfficient.transformers.modeling_outputs"]], "QEfficient.transformers.modeling_utils module": [[9, "module-QEfficient.transformers.modeling_utils"]], "Perfromance details": [[10, "perfromance-details"]], "Quick Start Guide": [[11, "quick-start-guide"]], "Using High Level API": [[11, "using-high-level-api"]], "1. Use QEfficient.cloud.infer": [[11, "use-qefficient-cloud-infer"]], "2. Use of QEfficient.cloud.execute": [[11, "use-of-qefficient-cloud-execute"]], "Using Low Level API": [[11, "using-low-level-api"]], "1.  Model download and transform": [[11, "model-download-and-transform"]], "2. ONNX export of transformed model": [[11, "onnx-export-of-transformed-model"]], "3. Compile on Cloud AI 100": [[11, "compile-on-cloud-ai-100"]], "4. Run Benchmark": [[11, "run-benchmark"]], "Qualcomm Cloud AI home": [[12, "qualcomm-cloud-ai-home"]], "Qualcomm Cloud AI SDK download": [[12, "qualcomm-cloud-ai-sdk-download"]], "Qualcomm Cloud AI API reference": [[12, "qualcomm-cloud-ai-api-reference"]], "User Guide": [[12, "user-guide"]], "OCP Microscaling Formats (MX) Specification": [[12, "ocp-microscaling-formats-mx-specification"]], "Validation of Model before and After KV Cache Optimization": [[13, "validation-of-model-before-and-after-kv-cache-optimization"]], "Run APIs": [[13, "run-apis"]], "Sample Usage :": [[13, "sample-usage"]], "Low level API": [[8, "low-level-api"]], "QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM": [[8, "module-QEfficient.src._transformers.auto"]], "QEfficient.exporter.export_hf_to_cloud_ai_100 module": [[8, "module-QEfficient.exporter.export_hf_to_cloud_ai_100"]], "API to convert torch Bert style and KV style model to ONNX.": [[8, "api-to-convert-torch-bert-style-and-kv-style-model-to-onnx"]], "QEfficient.compile": [[8, "module-QEfficient.compile.compile_helper"]], "API to compile the Onnx Model on Cloud AI 100 platform with given config.": [[8, "api-to-compile-the-onnx-model-on-cloud-ai-100-platform-with-given-config"]], "QEfficient.generation.text_generation_inference module": [[8, "module-QEfficient.generation.text_generation_inference"]], "API to execute QEfficient transformed ONNX model on Cloud AI 100 using compiled QPC file.": [[8, "api-to-execute-qefficient-transformed-onnx-model-on-cloud-ai-100-using-compiled-qpc-file"]], "API to execute Bertstyle ONNX model on Cloud AI 100.": [[8, "api-to-execute-bertstyle-onnx-model-on-cloud-ai-100"]]}, "indexentries": {"qeffautomodelforcausallm (class in qefficient.src._transformers.auto)": [[8, "QEfficient.src._transformers.auto.QEFFAutoModelForCausalLM"]], "qefficient.compile.compile_helper": [[8, "module-QEfficient.compile.compile_helper"]], "qefficient.exporter.export_hf_to_cloud_ai_100": [[8, "module-QEfficient.exporter.export_hf_to_cloud_ai_100"]], "qefficient.generation.text_generation_inference": [[8, "module-QEfficient.generation.text_generation_inference"]], "qefficient.src._transformers.auto": [[8, "module-QEfficient.src._transformers.auto"]], "cloud_ai_100_exec_kv() (in module qefficient.generation.text_generation_inference)": [[8, "QEfficient.generation.text_generation_inference.cloud_ai_100_exec_kv"]], "compile() (in module qefficient.compile.compile_helper)": [[8, "QEfficient.compile.compile_helper.compile"]], "convert_to_cloud_bertstyle() (in module qefficient.exporter.export_hf_to_cloud_ai_100)": [[8, "QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_bertstyle"]], "convert_to_cloud_kvstyle() (in module qefficient.exporter.export_hf_to_cloud_ai_100)": [[8, "QEfficient.exporter.export_hf_to_cloud_ai_100.convert_to_cloud_kvstyle"]], "latency_stats_bertstyle() (in module qefficient.generation.text_generation_inference)": [[8, "QEfficient.generation.text_generation_inference.latency_stats_bertstyle"]], "module": [[8, "module-QEfficient.compile.compile_helper"], [8, "module-QEfficient.exporter.export_hf_to_cloud_ai_100"], [8, "module-QEfficient.generation.text_generation_inference"], [8, "module-QEfficient.src._transformers.auto"]], "qualcomm_efficient_converter() (in module qefficient.exporter.export_hf_to_cloud_ai_100)": [[8, "QEfficient.exporter.export_hf_to_cloud_ai_100.qualcomm_efficient_converter"]]}})